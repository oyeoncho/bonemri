{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KM stratification using DCA-selected model (file04/run06) - NO DCA export\n",
    "# Mixture Stretched Exponential Survival + Model Saving + (Optional) External Eval + KM plots\n",
    "#\n",
    "# Key behavior:\n",
    "# - Train + save 30 Monte-Carlo models per feature-set (same as before)\n",
    "# - Then load the representative model (file04, run06) and draw KM plots\n",
    "# - External KM uses INTERNAL (validation) median cutoff (no external re-median)\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "BASE_GROUPS = [\"beit0\"]\n",
    "GROUP = \"n7_30_30\"\n",
    "N_RUNS = 30\n",
    "time_points = [12, 24, 36, 48, 60, 72]\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# reproducibility\n",
    "BASE_SEED = 20250903\n",
    "\n",
    "# model save root\n",
    "os.makedirs(\"./survival_model/mixture_non_fix/models\", exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class MixtureStretchedExponentialSurvival(nn.Module):\n",
    "    def __init__(self, input_dim, num_components=2):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU()\n",
    "        )\n",
    "        self.pi_layer = nn.Linear(64, num_components)\n",
    "        self.lam_layer = nn.Linear(64, num_components)\n",
    "        self.alpha_layer = nn.Linear(64, num_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        pi = F.softmax(self.pi_layer(h), dim=1)\n",
    "        lam = F.softplus(self.lam_layer(h)) + 1e-3\n",
    "        a = F.softplus(self.alpha_layer(h)) + 1e-3\n",
    "        return pi, lam, a\n",
    "\n",
    "def mixture_stretched_nll(t, e, pi, lam, a, eps=1e-8):\n",
    "    t = t.view(-1, 1)\n",
    "    t_a = torch.pow(t + eps, a)\n",
    "    S_k = torch.exp(-lam * t_a)\n",
    "    f_k = lam * a * torch.pow(t + eps, a - 1) * S_k\n",
    "    f = torch.sum(pi * f_k, dim=1) + eps\n",
    "    S = torch.sum(pi * S_k, dim=1) + eps\n",
    "    loglik = e * torch.log(f) + (1 - e) * torch.log(S)\n",
    "    return -loglik.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_survival(model, x, times):\n",
    "    model.eval()\n",
    "    pi, lam, a = model(x)\n",
    "    surv = []\n",
    "    for t in times:\n",
    "        t_tensor = torch.tensor([t], dtype=torch.float32, device=x.device)\n",
    "        t_a = torch.pow(t_tensor + 1e-8, a)\n",
    "        S_k = torch.exp(-lam * t_a)\n",
    "        S = torch.sum(pi * S_k, dim=1)\n",
    "        surv.append(S.cpu().numpy())\n",
    "    return np.vstack(surv)  # (T, N)\n",
    "\n",
    "def calc_auc(surv_arr, y_df, times):\n",
    "    aucs = {}\n",
    "    for i, t in enumerate(times):\n",
    "        true = ((y_df[\"event\"] == 1) & (y_df[\"time\"] <= t)).astype(int)\n",
    "        pred = 1 - surv_arr[i, :]\n",
    "        try:\n",
    "            aucs[t] = roc_auc_score(true, pred)\n",
    "        except Exception:\n",
    "            aucs[t] = np.nan\n",
    "    return aucs\n",
    "\n",
    "def safe_concordance_index(times, risks, events):\n",
    "    times = np.asarray(times)\n",
    "    risks = np.asarray(risks)\n",
    "    events = np.asarray(events)\n",
    "    mask = ~(np.isnan(times) | np.isnan(risks) | np.isnan(events))\n",
    "    if np.sum(mask) < 2:\n",
    "        return np.nan\n",
    "    if np.std(risks[mask]) < 1e-6:\n",
    "        return np.nan\n",
    "    return concordance_index(times[mask], risks[mask], events[mask])\n",
    "\n",
    "def save_model_and_ct(model_state, ct, save_dir, run_idx, label, input_dim):\n",
    "    tag = f\"run{run_idx:02d}_{label.replace(' ', '_')}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save({\"state_dict\": model_state, \"input_dim\": input_dim},\n",
    "               os.path.join(save_dir, f\"best_model_{tag}.pt\"))\n",
    "    joblib.dump(ct, os.path.join(save_dir, f\"ct_{tag}.joblib\"))\n",
    "\n",
    "def load_model_and_ct(model_dir, run_idx, label, device=DEVICE, num_components=2):\n",
    "    tag = f\"run{run_idx:02d}_{label.replace(' ', '_')}\"\n",
    "    ckpt_path = os.path.join(model_dir, f\"best_model_{tag}.pt\")\n",
    "    ct_path   = os.path.join(model_dir, f\"ct_{tag}.joblib\")\n",
    "\n",
    "    if (not os.path.exists(ckpt_path)) or (not os.path.exists(ct_path)):\n",
    "        raise FileNotFoundError(f\"Missing: {ckpt_path} or {ct_path}\")\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    input_dim = ckpt[\"input_dim\"]\n",
    "\n",
    "    model = MixtureStretchedExponentialSurvival(input_dim=input_dim, num_components=num_components).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        ct = joblib.load(ct_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(ct_path) from e\n",
    "\n",
    "    return model, ct\n",
    "\n",
    "def evaluate_external_for_all_models(MODEL_DIR, EXTERNAL_CSV, time_points, device=DEVICE):\n",
    "    \"\"\"Optional: external AUC/C-index for all saved models in MODEL_DIR.\"\"\"\n",
    "    if not os.path.exists(EXTERNAL_CSV):\n",
    "        print(f\"‚ÑπÔ∏è External eval skipped (missing): {EXTERNAL_CSV}\")\n",
    "        return None, None\n",
    "\n",
    "    df_ext = pd.read_csv(EXTERNAL_CSV)\n",
    "    if 'event' not in df_ext.columns and 'survival' in df_ext.columns:\n",
    "        df_ext['event'] = df_ext['survival'].astype(int)\n",
    "    if 'time' not in df_ext.columns and 'fu_date' in df_ext.columns:\n",
    "        df_ext['time'] = df_ext['fu_date'].astype(np.float32)\n",
    "\n",
    "    ckpt_paths = glob.glob(os.path.join(MODEL_DIR, \"best_model_run*.pt\"))\n",
    "    if not ckpt_paths:\n",
    "        print(f\"‚ö†Ô∏è No models in {MODEL_DIR}\")\n",
    "        return None, None\n",
    "\n",
    "    pattern = re.compile(r\"best_model_run(\\d+)_([^\\.]+)\\.pt\")\n",
    "    ext_rows_auc_all, ext_rows_cidx_all = [], []\n",
    "\n",
    "    for ckpt_path in sorted(ckpt_paths):\n",
    "        m = pattern.search(os.path.basename(ckpt_path))\n",
    "        if not m:\n",
    "            continue\n",
    "        run_idx = int(m.group(1))\n",
    "        label_raw = m.group(2)\n",
    "        label = label_raw.replace(\"_\", \" \")\n",
    "\n",
    "        try:\n",
    "            model, ct = load_model_and_ct(MODEL_DIR, run_idx, label, device=device, num_components=2)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "        required_columns = []\n",
    "        for name, trans, cols in ct.transformers_:\n",
    "            if cols is None or cols == []:\n",
    "                continue\n",
    "            if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "                required_columns.extend(list(cols))\n",
    "            else:\n",
    "                required_columns.append(cols)\n",
    "\n",
    "        missing = [c for c in required_columns if c not in df_ext.columns]\n",
    "        if missing:\n",
    "            continue\n",
    "\n",
    "        X_ext = ct.transform(df_ext[required_columns])\n",
    "        X_ext = pd.DataFrame(X_ext).fillna(0).values.astype(np.float32)\n",
    "        X_ext_tensor = torch.tensor(X_ext, dtype=torch.float32).to(device)\n",
    "\n",
    "        surv_ext = predict_survival(model, X_ext_tensor, time_points)\n",
    "        y_ext = df_ext[['time', 'event']].copy()\n",
    "\n",
    "        auc_ext = calc_auc(surv_ext, y_ext.reset_index(drop=True), time_points)\n",
    "\n",
    "        # NOTE: This keeps your original \"risk = S(t)\" direction for C-index.\n",
    "        # For strict risk direction you may prefer risk = 1 - S(t) or -S(t).\n",
    "        risk_ext = surv_ext.T\n",
    "        cidx_ext = [safe_concordance_index(y_ext['time'], risk_ext[:, j], y_ext['event'])\n",
    "                    for j in range(len(time_points))]\n",
    "\n",
    "        for j, t in enumerate(time_points):\n",
    "            ext_rows_auc_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                     \"Time (Months)\": t, \"AUC (External)\": auc_ext[t]})\n",
    "            ext_rows_cidx_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                      \"Time (Months)\": t, \"C-index (External)\": cidx_ext[j]})\n",
    "\n",
    "    return ext_rows_auc_all, ext_rows_cidx_all\n",
    "\n",
    "# =========================\n",
    "# KM helper (fixed cutoff support)\n",
    "# =========================\n",
    "def make_km_plot(df_time_event_risk: pd.DataFrame,\n",
    "                title: str,\n",
    "                out_png: str,\n",
    "                out_summary_csv: str,\n",
    "                fixed_cut: float = None):\n",
    "    \"\"\"\n",
    "    df_time_event_risk must have columns: time, event, risk\n",
    "    If fixed_cut is provided, use it for High/Low split (e.g., internal median applied to external).\n",
    "    Otherwise, uses median of df['risk'].\n",
    "    Returns the cutoff used.\n",
    "    \"\"\"\n",
    "    df = df_time_event_risk.copy()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"time\", \"event\", \"risk\"])\n",
    "    if len(df) < 4:\n",
    "        print(f\"‚ö†Ô∏è Too few samples for KM: {out_png}\")\n",
    "        return None\n",
    "\n",
    "    cut = float(fixed_cut) if fixed_cut is not None else float(np.nanmedian(df[\"risk\"].values))\n",
    "    df[\"risk_group\"] = np.where(df[\"risk\"] >= cut, \"High\", \"Low\")\n",
    "\n",
    "    summary = (df.groupby(\"risk_group\")\n",
    "                 .agg(n=(\"risk_group\", \"size\"),\n",
    "                      events=(\"event\", \"sum\"),\n",
    "                      median_risk=(\"risk\", \"median\"))\n",
    "                 .reset_index())\n",
    "    summary[\"cut_used\"] = cut\n",
    "    summary.to_csv(out_summary_csv, index=False)\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for g in [\"Low\", \"High\"]:\n",
    "        sub = df[df[\"risk_group\"] == g]\n",
    "        if len(sub) < 2:\n",
    "            continue\n",
    "        kmf.fit(sub[\"time\"], event_observed=sub[\"event\"], label=f\"{g} (n={len(sub)})\")\n",
    "        kmf.plot(ci_show=True)\n",
    "\n",
    "    low = df[df[\"risk_group\"] == \"Low\"]\n",
    "    high = df[df[\"risk_group\"] == \"High\"]\n",
    "    if (len(low) >= 2) and (len(high) >= 2):\n",
    "        lr = logrank_test(low[\"time\"], high[\"time\"],\n",
    "                          event_observed_A=low[\"event\"], event_observed_B=high[\"event\"])\n",
    "        pval = lr.p_value\n",
    "    else:\n",
    "        pval = np.nan\n",
    "\n",
    "    # ‚úÖ cutoff(Ï§ëÏïôÍ∞í risk) ÌëúÏãú: Ï†úÎ™© + figure text Îëò Îã§\n",
    "    title2 = f\"{title}\\ncutoff (median risk) = {cut:.4f} | log-rank p = {pval:.3g}\"\n",
    "    plt.title(title2)\n",
    "\n",
    "    # Í∑∏Î¶º ÏïàÏóêÎèÑ Ìïú Î≤à Îçî ÌëúÏãú(ÏõêÏπò ÏïäÏúºÎ©¥ Ïù¥ Î∏îÎ°ù ÏÇ≠Ï†ú)\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.02, 0.02, f\"cutoff = {cut:.4f}\", transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment=\"bottom\")\n",
    "\n",
    "    plt.xlabel(\"Time (Months)\")\n",
    "    plt.ylabel(\"Survival probability\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"‚úÖ KM saved: {out_png}\")\n",
    "    print(f\"‚úÖ KM summary saved: {out_summary_csv}\")\n",
    "    print(f\"   ‚Ü≥ cutoff (median risk) = {cut:.6f}\")\n",
    "    return cut\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main: train + save models (kept)\n",
    "# =========================\n",
    "device = DEVICE\n",
    "\n",
    "for base_group in BASE_GROUPS:\n",
    "    print(f\"\\n\\n============================\")\n",
    "    print(f\"üìÅ BEiT Í∑∏Î£π Ïã§Ìñâ Ï§ë: {base_group}\")\n",
    "    print(f\"============================\")\n",
    "\n",
    "    SAVE_ROOT_BASE = f\"./survival_model/mixture_non_fix/non_nest/{base_group}/results/generalization/test_km/dl0/{GROUP}\"\n",
    "    MODEL_DIR_ROOT = f\"./survival_model/mixture_non_fix/models/{base_group}/{GROUP}\"\n",
    "    os.makedirs(SAVE_ROOT_BASE, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR_ROOT, exist_ok=True)\n",
    "\n",
    "    img_cols = [\"feat_436\", \"feat_519\"]\n",
    "    cont_cols = [\"Age\"]\n",
    "    cat_cols  = [\"pathology\", \"stage0\"]\n",
    "\n",
    "    feature_sets = {\n",
    "        \"Image only\": (img_cols, []),\n",
    "        \"Clinical only\": ([], cont_cols + cat_cols),\n",
    "        \"Image + Clinical\": (img_cols, cont_cols + cat_cols),\n",
    "    }\n",
    "\n",
    "    # run only file04 (as your current setting)\n",
    "    for i in range(4, 5):\n",
    "        ONLY_RUN_IDX = i\n",
    "        EXTERNAL_CSV = f\"./external/external{ONLY_RUN_IDX}.csv\"\n",
    "\n",
    "        fname = f\"dh11_run{ONLY_RUN_IDX:02d}.csv\"\n",
    "        csv_path = f\"./deephit/{base_group}/test/dl0/{GROUP}/{fname}\"\n",
    "        print(f\"\\nüöÄ Training: {base_group} - {fname} | EXTERNAL: external{ONLY_RUN_IDX}.csv\")\n",
    "\n",
    "        MODEL_DIR_I = os.path.join(MODEL_DIR_ROOT, f\"file{ONLY_RUN_IDX:02d}\")\n",
    "        SAVE_ROOT   = os.path.join(SAVE_ROOT_BASE, f\"file{ONLY_RUN_IDX:02d}\")\n",
    "        os.makedirs(MODEL_DIR_I, exist_ok=True)\n",
    "        os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ö†Ô∏è Missing internal data: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        df_all = pd.read_csv(csv_path)\n",
    "        if 'event' not in df_all.columns and 'survival' in df_all.columns:\n",
    "            df_all['event'] = df_all['survival'].astype(int)\n",
    "        if 'time' not in df_all.columns and 'fu_date' in df_all.columns:\n",
    "            df_all['time']  = df_all['fu_date'].astype(np.float32)\n",
    "\n",
    "        results_dict = {}\n",
    "        raw_rows_auc, raw_rows_cidx = [], []\n",
    "\n",
    "        for label, (img_part, clinical_part) in feature_sets.items():\n",
    "            print(f\"\\nüìå Feature Set: {label}\")\n",
    "\n",
    "            auc_train_list, auc_val_list = [], []\n",
    "            cidx_train_list, cidx_val_list = [], []\n",
    "\n",
    "            for run in range(N_RUNS):\n",
    "                set_seed(BASE_SEED + run)\n",
    "\n",
    "                used_cols = img_part + clinical_part\n",
    "                X_df = df_all[used_cols].copy()\n",
    "                y_df = df_all[[\"time\", \"event\"]].copy()\n",
    "\n",
    "                X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "                    X_df, y_df, test_size=0.3, random_state=BASE_SEED + run\n",
    "                )\n",
    "\n",
    "                transformers = []\n",
    "                if img_part:\n",
    "                    transformers.append((\"img\", StandardScaler(), img_part))\n",
    "\n",
    "                cont = [c for c in clinical_part if c in cont_cols]\n",
    "                cat  = [c for c in clinical_part if c in cat_cols]\n",
    "                if cont:\n",
    "                    transformers.append((\"cont\", StandardScaler(), cont))\n",
    "                if cat:\n",
    "                    transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cat))\n",
    "\n",
    "                ct = ColumnTransformer(transformers)\n",
    "\n",
    "                X_train = ct.fit_transform(X_train_df)\n",
    "                X_val   = ct.transform(X_val_df)\n",
    "\n",
    "                X_train = pd.DataFrame(X_train).fillna(0).values.astype(np.float32)\n",
    "                X_val   = pd.DataFrame(X_val).fillna(0).values.astype(np.float32)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "                X_val_tensor   = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "\n",
    "                t_train = torch.tensor(y_train[\"time\"].values, dtype=torch.float32).to(device)\n",
    "                e_train = torch.tensor(y_train[\"event\"].values, dtype=torch.float32).to(device)\n",
    "\n",
    "                model = MixtureStretchedExponentialSurvival(input_dim=X_train.shape[1], num_components=2).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "                best_loss = float(\"inf\")\n",
    "                patience, patience_counter = 10, 0\n",
    "                best_model_state = None\n",
    "\n",
    "                for epoch in range(1000):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    pi, lam, a = model(X_train_tensor)\n",
    "                    loss = mixture_stretched_nll(t_train, e_train, pi, lam, a)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # early stop on train loss (as original)\n",
    "                    if loss.item() < best_loss - 1e-6:\n",
    "                        best_loss = loss.item()\n",
    "                        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    save_model_and_ct(best_model_state, ct, MODEL_DIR_I, run, label, input_dim=X_train.shape[1])\n",
    "\n",
    "                # internal eval (kept)\n",
    "                surv_train = predict_survival(model, X_train_tensor, time_points)\n",
    "                surv_val   = predict_survival(model, X_val_tensor, time_points)\n",
    "\n",
    "                auc_train = calc_auc(surv_train, y_train.reset_index(drop=True), time_points)\n",
    "                auc_val   = calc_auc(surv_val,   y_val.reset_index(drop=True), time_points)\n",
    "                auc_train_list.append(auc_train)\n",
    "                auc_val_list.append(auc_val)\n",
    "\n",
    "                # NOTE: keeps your original direction for C-index evaluation\n",
    "                risk_train = surv_train.T\n",
    "                risk_val   = surv_val.T\n",
    "                cidx_train = [safe_concordance_index(y_train[\"time\"], risk_train[:, j], y_train[\"event\"])\n",
    "                              for j in range(len(time_points))]\n",
    "                cidx_val   = [safe_concordance_index(y_val[\"time\"],   risk_val[:,   j], y_val[\"event\"])\n",
    "                              for j in range(len(time_points))]\n",
    "                cidx_train_list.append(cidx_train)\n",
    "                cidx_val_list.append(cidx_val)\n",
    "\n",
    "                for j, t in enumerate(time_points):\n",
    "                    raw_rows_auc.append({\n",
    "                        \"Feature Set\": label, \"Run\": run, \"Time (Months)\": t,\n",
    "                        \"AUC (Train)\": auc_train[t], \"AUC (Val)\": auc_val[t], \"Scope\": \"Time-wise\"\n",
    "                    })\n",
    "                    raw_rows_cidx.append({\n",
    "                        \"Feature Set\": label, \"Run\": run, \"Time (Months)\": t,\n",
    "                        \"C-index (Train)\": cidx_train[j], \"C-index (Val)\": cidx_val[j], \"Scope\": \"Time-wise\"\n",
    "                    })\n",
    "\n",
    "            results_dict[label] = {\n",
    "                \"mean_auc_train\": {t: np.nanmean([r[t] for r in auc_train_list]) for t in time_points},\n",
    "                \"mean_auc_val\":   {t: np.nanmean([r[t] for r in auc_val_list])   for t in time_points},\n",
    "                \"std_auc_train\":  {t: np.nanstd([r[t] for r in auc_train_list])  for t in time_points},\n",
    "                \"std_auc_val\":    {t: np.nanstd([r[t] for r in auc_val_list])    for t in time_points},\n",
    "            }\n",
    "\n",
    "        # save internal eval csv (kept)\n",
    "        raw_auc_path  = os.path.join(SAVE_ROOT, f\"raw_auc_per_time_run{ONLY_RUN_IDX:02d}.csv\")\n",
    "        raw_cidx_path = os.path.join(SAVE_ROOT, f\"raw_cindex_per_time_run{ONLY_RUN_IDX:02d}.csv\")\n",
    "        pd.DataFrame(raw_rows_auc).to_csv(raw_auc_path, index=False)\n",
    "        pd.DataFrame(raw_rows_cidx).to_csv(raw_cidx_path, index=False)\n",
    "        print(f\"‚úÖ Internal eval saved: run{ONLY_RUN_IDX:02d}\")\n",
    "\n",
    "        # optional external eval (kept)\n",
    "        ext_auc_rows, ext_cidx_rows = evaluate_external_for_all_models(\n",
    "            MODEL_DIR=MODEL_DIR_I,\n",
    "            EXTERNAL_CSV=EXTERNAL_CSV,\n",
    "            time_points=time_points,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        if ext_auc_rows:\n",
    "            pd.DataFrame(ext_auc_rows).to_csv(\n",
    "                os.path.join(SAVE_ROOT, f\"external_auc_ALL_runs_from_file{ONLY_RUN_IDX:02d}.csv\"),\n",
    "                index=False\n",
    "            )\n",
    "        if ext_cidx_rows:\n",
    "            pd.DataFrame(ext_cidx_rows).to_csv(\n",
    "                os.path.join(SAVE_ROOT, f\"external_cindex_ALL_runs_from_file{ONLY_RUN_IDX:02d}.csv\"),\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "# =========================\n",
    "# KM ONLY: representative model (file04, run06) and KM plots\n",
    "# =========================\n",
    "REP_FILE_IDX = 4\n",
    "REP_RUN_IDX  = 6\n",
    "REP_FEATURES = [\"Image only\", \"Clinical only\", \"Image + Clinical\"]\n",
    "T0 = 60\n",
    "\n",
    "base_group = BASE_GROUPS[0]\n",
    "\n",
    "MODEL_DIR_ROOT = f\"./survival_model/mixture_non_fix/models/{base_group}/{GROUP}\"\n",
    "MODEL_DIR_I    = os.path.join(MODEL_DIR_ROOT, f\"file{REP_FILE_IDX:02d}\")\n",
    "\n",
    "DATA_CSV_PATH      = f\"./deephit/{base_group}/test/dl0/{GROUP}/dh11_run{REP_FILE_IDX:02d}.csv\"\n",
    "EXTERNAL_CSV_PATH  = f\"./external/external{REP_FILE_IDX}.csv\"\n",
    "\n",
    "SAVE_ROOT_BASE = f\"./survival_model/mixture_non_fix/non_nest/{base_group}/results/generalization/test_km/dl0/{GROUP}\"\n",
    "SAVE_ROOT_I    = os.path.join(SAVE_ROOT_BASE, f\"file{REP_FILE_IDX:02d}\")\n",
    "os.makedirs(SAVE_ROOT_I, exist_ok=True)\n",
    "\n",
    "print(\"\\n============================================\")\n",
    "print(f\"üìå KM ONLY: FILE run{REP_FILE_IDX:02d}, MODEL run{REP_RUN_IDX:02d}, T0={T0}\")\n",
    "print(\"============================================\")\n",
    "\n",
    "# internal data\n",
    "if not os.path.exists(DATA_CSV_PATH):\n",
    "    raise FileNotFoundError(f\"Missing internal data: {DATA_CSV_PATH}\")\n",
    "df_all = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "if 'event' not in df_all.columns and 'survival' in df_all.columns:\n",
    "    df_all['event'] = df_all['survival'].astype(int)\n",
    "if 'time' not in df_all.columns and 'fu_date' in df_all.columns:\n",
    "    df_all['time'] = df_all['fu_date'].astype(np.float32)\n",
    "\n",
    "# external data (optional)\n",
    "df_ext = None\n",
    "if os.path.exists(EXTERNAL_CSV_PATH):\n",
    "    df_ext = pd.read_csv(EXTERNAL_CSV_PATH)\n",
    "    if 'event' not in df_ext.columns and 'survival' in df_ext.columns:\n",
    "        df_ext['event'] = df_ext['survival'].astype(int)\n",
    "    if 'time' not in df_ext.columns and 'fu_date' in df_ext.columns:\n",
    "        df_ext['time'] = df_ext['fu_date'].astype(np.float32)\n",
    "\n",
    "# feature definitions\n",
    "img_cols = [\"feat_436\", \"feat_519\"]\n",
    "cont_cols = [\"Age\"]\n",
    "cat_cols  = [\"pathology\", \"stage0\"]\n",
    "\n",
    "for REP_FEATURE in REP_FEATURES:\n",
    "    print(f\"\\n‚û°Ô∏è KM: Feature Set = {REP_FEATURE}\")\n",
    "\n",
    "    if REP_FEATURE == \"Image only\":\n",
    "        used_cols = img_cols\n",
    "    elif REP_FEATURE == \"Clinical only\":\n",
    "        used_cols = cont_cols + cat_cols\n",
    "    elif REP_FEATURE == \"Image + Clinical\":\n",
    "        used_cols = img_cols + cont_cols + cat_cols\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # reproduce same split as run06\n",
    "    X_df = df_all[used_cols].copy()\n",
    "    y_df = df_all[[\"time\", \"event\"]].copy()\n",
    "\n",
    "    set_seed(BASE_SEED + REP_RUN_IDX)\n",
    "    X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "        X_df, y_df, test_size=0.3, random_state=BASE_SEED + REP_RUN_IDX\n",
    "    )\n",
    "\n",
    "    # load representative model + ct\n",
    "    try:\n",
    "        model, ct = load_model_and_ct(\n",
    "            MODEL_DIR_I, REP_RUN_IDX, REP_FEATURE, device=DEVICE, num_components=2\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Missing model/ct: {MODEL_DIR_I} run{REP_RUN_IDX:02d} {REP_FEATURE}\")\n",
    "        continue\n",
    "\n",
    "    # -------- Internal validation: risk = 1 - S(T0)\n",
    "    X_val = ct.transform(X_val_df)\n",
    "    X_val = pd.DataFrame(X_val).fillna(0).values.astype(np.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    surv_val_T0 = predict_survival(model, X_val_tensor, [T0])[0, :]\n",
    "    risk_val = 1.0 - surv_val_T0\n",
    "\n",
    "    df_km_internal = pd.DataFrame({\n",
    "        \"time\": y_val[\"time\"].values,\n",
    "        \"event\": y_val[\"event\"].values.astype(int),\n",
    "        \"risk\": risk_val,\n",
    "    })\n",
    "\n",
    "    out_png_int = os.path.join(\n",
    "        SAVE_ROOT_I,\n",
    "        f\"km_internal_file{REP_FILE_IDX:02d}_run{REP_RUN_IDX:02d}_{REP_FEATURE.replace(' ', '_')}_T{T0}.png\"\n",
    "    )\n",
    "    out_sum_int = os.path.join(\n",
    "        SAVE_ROOT_I,\n",
    "        f\"km_internal_summary_file{REP_FILE_IDX:02d}_run{REP_RUN_IDX:02d}_{REP_FEATURE.replace(' ', '_')}_T{T0}.csv\"\n",
    "    )\n",
    "\n",
    "    # internal median cutoff (store it)\n",
    "    internal_cut = make_km_plot(\n",
    "        df_km_internal,\n",
    "        title=f\"KM (Internal Validation) by Median Predicted Risk @ {T0} mo\\n{REP_FEATURE} | file{REP_FILE_IDX:02d} run{REP_RUN_IDX:02d}\",\n",
    "        out_png=out_png_int,\n",
    "        out_summary_csv=out_sum_int,\n",
    "        fixed_cut=None\n",
    "    )\n",
    "\n",
    "    # -------- External: apply INTERNAL cutoff\n",
    "    if df_ext is None:\n",
    "        print(\"   ‚ÑπÔ∏è External missing ‚Üí skip external KM\")\n",
    "        continue\n",
    "\n",
    "    missing = [c for c in used_cols if c not in df_ext.columns]\n",
    "    if missing:\n",
    "        print(f\"   ‚ö†Ô∏è External missing columns for {REP_FEATURE}: {missing} ‚Üí skip external KM\")\n",
    "        continue\n",
    "\n",
    "    X_ext_df = df_ext[used_cols].copy()\n",
    "    X_ext = ct.transform(X_ext_df)\n",
    "    X_ext = pd.DataFrame(X_ext).fillna(0).values.astype(np.float32)\n",
    "    X_ext_tensor = torch.tensor(X_ext, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    surv_ext_T0 = predict_survival(model, X_ext_tensor, [T0])[0, :]\n",
    "    risk_ext = 1.0 - surv_ext_T0\n",
    "\n",
    "    df_km_external = pd.DataFrame({\n",
    "        \"time\": df_ext[\"time\"].values,\n",
    "        \"event\": df_ext[\"event\"].values.astype(int),\n",
    "        \"risk\": risk_ext,\n",
    "    })\n",
    "\n",
    "    out_png_ext = os.path.join(\n",
    "        SAVE_ROOT_I,\n",
    "        f\"km_external_file{REP_FILE_IDX:02d}_run{REP_RUN_IDX:02d}_{REP_FEATURE.replace(' ', '_')}_T{T0}.png\"\n",
    "    )\n",
    "    out_sum_ext = os.path.join(\n",
    "        SAVE_ROOT_I,\n",
    "        f\"km_external_summary_file{REP_FILE_IDX:02d}_run{REP_RUN_IDX:02d}_{REP_FEATURE.replace(' ', '_')}_T{T0}.csv\"\n",
    "    )\n",
    "\n",
    "    make_km_plot(\n",
    "        df_km_external,\n",
    "        title=f\"KM (External) by INTERNAL Cutoff @ {T0} mo\\n{REP_FEATURE} | file{REP_FILE_IDX:02d} run{REP_RUN_IDX:02d}\",\n",
    "        out_png=out_png_ext,\n",
    "        out_summary_csv=out_sum_ext,\n",
    "        fixed_cut=internal_cut  # ‚úÖ internal median applied to external\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
