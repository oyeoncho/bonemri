{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3702dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OUT] ./visualization/all_models/VP005_pre_0000_RUNS0_29_HOTP50_70_90_run3_specific_freqLE2\n",
      "[MEMMAP] ./visualization/all_models/VP005_pre_0000_RUNS0_29_HOTP50_70_90_run3_specific_freqLE2/VP005_pre_0000_heat30runs_float32.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute heatmaps (30 runs): 100%|██████████| 30/30 [20:19<00:00, 40.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== HOT_P = 50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hotspot freq (p50): 100%|██████████| 30/30 [00:00<00:00, 115.12it/s]\n",
      "Metrics (run vs consensus) p50: 100%|██████████| 30/30 [00:01<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] CSV: ./visualization/all_models/VP005_pre_0000_RUNS0_29_HOTP50_70_90_run3_specific_freqLE2/VP005_pre_0000_metrics_30runs_vs_consensus_p50.csv\n",
      "\n",
      "===== HOT_P = 70 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hotspot freq (p70): 100%|██████████| 30/30 [00:00<00:00, 116.76it/s]\n",
      "Metrics (run vs consensus) p70: 100%|██████████| 30/30 [00:01<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] CSV: ./visualization/all_models/VP005_pre_0000_RUNS0_29_HOTP50_70_90_run3_specific_freqLE2/VP005_pre_0000_metrics_30runs_vs_consensus_p70.csv\n",
      "\n",
      "===== HOT_P = 90 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hotspot freq (p90): 100%|██████████| 30/30 [00:00<00:00, 123.91it/s]\n",
      "Metrics (run vs consensus) p90: 100%|██████████| 30/30 [00:01<00:00, 18.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] CSV: ./visualization/all_models/VP005_pre_0000_RUNS0_29_HOTP50_70_90_run3_specific_freqLE2/VP005_pre_0000_metrics_30runs_vs_consensus_p90.csv\n",
      "\n",
      "✅ DONE (all percentiles)\n",
      "Output folder: ./visualization/all_models/VP005_pre_0000_RUNS0_29_HOTP50_70_90_run3_specific_freqLE2\n",
      "\n",
      "[Key outputs]\n",
      " - MRI: VP005_pre_0000_mri_masked_224.nii.gz\n",
      " - consensus heat: VP005_pre_0000_CONSENSUS_mean_raw.nii.gz\n",
      " - hotspot freq p50: VP005_pre_0000_HOT_FREQ_p50_count.nii.gz\n",
      " - consensus hot p50: VP005_pre_0000_CONSENSUS_HOT_p50.nii.gz\n",
      " - run3 unique p50: VP005_pre_0000_run3_UNIQUE_p50_freqLE2.nii.gz\n",
      " - run3 unique heat p50: VP005_pre_0000_run3_UNIQUE_heat_raw_p50.nii.gz\n",
      " - CSV p50: VP005_pre_0000_metrics_30runs_vs_consensus_p50.csv\n",
      " - hotspot freq p70: VP005_pre_0000_HOT_FREQ_p70_count.nii.gz\n",
      " - consensus hot p70: VP005_pre_0000_CONSENSUS_HOT_p70.nii.gz\n",
      " - run3 unique p70: VP005_pre_0000_run3_UNIQUE_p70_freqLE2.nii.gz\n",
      " - run3 unique heat p70: VP005_pre_0000_run3_UNIQUE_heat_raw_p70.nii.gz\n",
      " - CSV p70: VP005_pre_0000_metrics_30runs_vs_consensus_p70.csv\n",
      " - hotspot freq p90: VP005_pre_0000_HOT_FREQ_p90_count.nii.gz\n",
      " - consensus hot p90: VP005_pre_0000_CONSENSUS_HOT_p90.nii.gz\n",
      " - run3 unique p90: VP005_pre_0000_run3_UNIQUE_p90_freqLE2.nii.gz\n",
      " - run3 unique heat p90: VP005_pre_0000_run3_UNIQUE_heat_raw_p90.nii.gz\n",
      " - CSV p90: VP005_pre_0000_metrics_30runs_vs_consensus_p90.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "CACHE_SLICE_DIR = \"./radiomics_ready/output_224/images\"\n",
    "NIFTI_NAME = \"VP005_pre_0000.nii.gz\"\n",
    "MASK_PATH  = \"./radiomics_ready/output_224/masks/VP005_pre_0000.nii.gz\"\n",
    "\n",
    "# 모델 폴더 + 패턴 (run0~run29)\n",
    "MODEL_DIR = \"/Users/oyeoncho/Dropbox/working_directory/prep_cx_ca_image/image_data/models_extract/beit0\"\n",
    "MODEL_PATTERN = \"best_model_beit_run{run}.pt\"\n",
    "RUNS = list(range(30))  # run0~run29 (총 30개)\n",
    "\n",
    "OUT_DIR = \"./visualization/all_models\"\n",
    "TARGET_XY = (224, 224)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "INTEGRATED_FEATS = [436, 519]\n",
    "\n",
    "# ✅ 여러 hotspot percentile\n",
    "HOT_P_LIST = [50, 70, 90]  # 원하면 [90,95,97,99] 등으로 바꾸면 됨\n",
    "\n",
    "# run3 특이부위: run4 hotspot인데, 다른 run들에서 hotspot 빈도가 <= FREQ_MAX 인 voxel\n",
    "RUN_TARGET = 3\n",
    "FREQ_MAX = 2  # 더 엄격하게: 0~1, 덜 엄격하게: 3~5\n",
    "\n",
    "# 저장 옵션\n",
    "SAVE_MRI_MASKED_NIFTI = True\n",
    "SAVE_ALL_RUN_HEAT_NIFTI = False  # True면 run0~29 heatmap도 전부 NIfTI로 저장(용량 큼)\n",
    "\n",
    "# 메모리 절약: 모든 run heat를 디스크 memmap으로 저장\n",
    "USE_MEMMAP = True\n",
    "\n",
    "# =========================================================\n",
    "# MODEL\n",
    "# =========================================================\n",
    "class BEiTBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"beit_base_patch16_224\", pretrained=False, num_classes=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "def load_beit_backbone(model_path: str, device):\n",
    "    model = BEiTBackbone().to(device)\n",
    "    try:\n",
    "        with torch.serialization.safe_globals([np._core.multiarray.scalar]):\n",
    "            ckpt = torch.load(model_path, map_location=device)\n",
    "    except Exception:\n",
    "        ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "    state_dict = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "    filtered = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"backbone.\")}\n",
    "    if len(filtered) == 0:\n",
    "        filtered = state_dict\n",
    "\n",
    "    model.backbone.load_state_dict(filtered, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# NIfTI I/O\n",
    "# =========================================================\n",
    "def load_nifti_as_zyx(nifti_path: str):\n",
    "    img = sitk.ReadImage(nifti_path)\n",
    "    arr = sitk.GetArrayFromImage(img).astype(np.float32)  # (Z, Y, X)\n",
    "    return img, arr\n",
    "\n",
    "def resample_xy_keep_z(sitk_img, target_xy=(224,224), interp=sitk.sitkLinear):\n",
    "    size = list(sitk_img.GetSize())     # (X, Y, Z)\n",
    "    spacing = list(sitk_img.GetSpacing())\n",
    "\n",
    "    new_size = [int(target_xy[0]), int(target_xy[1]), int(size[2])]\n",
    "    new_spacing = [\n",
    "        spacing[0] * (size[0] / new_size[0]),\n",
    "        spacing[1] * (size[1] / new_size[1]),\n",
    "        spacing[2]\n",
    "    ]\n",
    "\n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetSize(new_size)\n",
    "    resampler.SetOutputSpacing(new_spacing)\n",
    "    resampler.SetOutputOrigin(sitk_img.GetOrigin())\n",
    "    resampler.SetOutputDirection(sitk_img.GetDirection())\n",
    "    resampler.SetInterpolator(interp)\n",
    "    return resampler.Execute(sitk_img)\n",
    "\n",
    "def save_zyx_as_nifti(vol_zyx: np.ndarray, ref_img_sitk, out_path: str):\n",
    "    out = sitk.GetImageFromArray(vol_zyx.astype(np.float32))  # expects (Z,Y,X)\n",
    "    out.SetSpacing(ref_img_sitk.GetSpacing())\n",
    "    out.SetOrigin(ref_img_sitk.GetOrigin())\n",
    "    out.SetDirection(ref_img_sitk.GetDirection())\n",
    "    sitk.WriteImage(out, out_path)\n",
    "\n",
    "# =========================================================\n",
    "# Preprocess\n",
    "# =========================================================\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1,3,1,1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1,3,1,1)\n",
    "\n",
    "def volume_to_tensors(arr_zyx: np.ndarray):\n",
    "    t = torch.from_numpy(arr_zyx).float()\n",
    "    vmin = float(t.min().item()); vmax = float(t.max().item())\n",
    "    if vmax > vmin:\n",
    "        t01 = (t - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        t01 = torch.zeros_like(t)\n",
    "\n",
    "    imgs_vis = t01.clone()                          # (Z,Y,X) in [0,1]\n",
    "    x = t01.unsqueeze(1).repeat(1, 3, 1, 1)         # (Z,3,Y,X)\n",
    "    x = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return x, imgs_vis\n",
    "\n",
    "def mask_to_binary(mask_zyx: np.ndarray, thr=0.5):\n",
    "    return (mask_zyx > thr).astype(np.uint8)\n",
    "\n",
    "def upsample_14_to_224(m14: torch.Tensor):\n",
    "    m = m14.unsqueeze(0).unsqueeze(0)  # (1,1,14,14)\n",
    "    up = F.interpolate(m, size=(224,224), mode=\"bilinear\", align_corners=False)[0,0]\n",
    "    return up\n",
    "\n",
    "# =========================================================\n",
    "# Integrated grad heatmap (raw) for one slice\n",
    "# =========================================================\n",
    "def compute_integrated_gradmap_one_slice(model: BEiTBackbone, x1: torch.Tensor, integrated_feats):\n",
    "    tokens = model.backbone.forward_features(x1)  # (1,1+N,C)\n",
    "    tokens.retain_grad()\n",
    "\n",
    "    if hasattr(model.backbone, \"forward_head\"):\n",
    "        feats = model.backbone.forward_head(tokens, pre_logits=True)  # (1,C)\n",
    "    else:\n",
    "        feats = tokens[:, 0]\n",
    "\n",
    "    patch_tok = tokens[:, 1:, :]  # (1,N,C)\n",
    "    n = patch_tok.shape[1]\n",
    "    h = w = int(np.sqrt(n))\n",
    "\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    if tokens.grad is not None:\n",
    "        tokens.grad.zero_()\n",
    "\n",
    "    target = sum(feats[:, idx].sum() for idx in integrated_feats)\n",
    "    target.backward(retain_graph=False)\n",
    "\n",
    "    patch_grad = tokens.grad[:, 1:, :]\n",
    "    imp14 = (patch_grad[0] * patch_tok[0]).abs().mean(dim=-1).reshape(h, w)  # (14,14)\n",
    "    imp224 = upsample_14_to_224(imp14).detach().cpu().numpy().astype(np.float32)\n",
    "    return imp224\n",
    "\n",
    "# =========================================================\n",
    "# Hotspot + metrics\n",
    "# =========================================================\n",
    "def hotspot_mask(H, p=95):\n",
    "    pos = H[H > 0]\n",
    "    if pos.size < 10:\n",
    "        return np.zeros_like(H, dtype=np.uint8), np.nan\n",
    "    thr = np.percentile(pos, p)\n",
    "    return (H >= thr).astype(np.uint8), float(thr)\n",
    "\n",
    "def bin_metrics(A, B):\n",
    "    A = (A > 0); B = (B > 0)\n",
    "    inter = np.logical_and(A, B).sum()\n",
    "    union = np.logical_or(A, B).sum()\n",
    "    a = A.sum(); b = B.sum()\n",
    "\n",
    "    dice = (2*inter) / (a + b + 1e-8)\n",
    "    iou  = inter / (union + 1e-8)\n",
    "\n",
    "    tp = inter\n",
    "    fp = np.logical_and(A, ~B).sum()\n",
    "    fn = np.logical_and(~A, B).sum()\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "    f1        = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"A_vox\": int(a), \"B_vox\": int(b),\n",
    "        \"inter\": int(inter), \"union\": int(union),\n",
    "        \"dice\": float(dice), \"iou\": float(iou),\n",
    "        \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1),\n",
    "        \"fp\": int(fp), \"fn\": int(fn),\n",
    "    }\n",
    "\n",
    "def centroid_distance(A, B, spacing_xyz=None):\n",
    "    A_idx = np.argwhere(A)\n",
    "    B_idx = np.argwhere(B)\n",
    "    if A_idx.size == 0 or B_idx.size == 0:\n",
    "        return np.nan\n",
    "    ca = A_idx.mean(axis=0)  # (Z,Y,X)\n",
    "    cb = B_idx.mean(axis=0)\n",
    "    d = ca - cb\n",
    "    if spacing_xyz is not None:\n",
    "        sx, sy, sz = spacing_xyz  # (X,Y,Z)\n",
    "        d = np.array([d[0]*sz, d[1]*sy, d[2]*sx], dtype=np.float32)\n",
    "    return float(np.linalg.norm(d))\n",
    "\n",
    "def cont_metrics(H1, H2, roi_mask):\n",
    "    x = H1[roi_mask].astype(np.float64)\n",
    "    y = H2[roi_mask].astype(np.float64)\n",
    "\n",
    "    x0 = x - x.mean()\n",
    "    y0 = y - y.mean()\n",
    "    corr = (x0*y0).sum() / (np.sqrt((x0*x0).sum() * (y0*y0).sum()) + 1e-12)\n",
    "\n",
    "    mae  = np.mean(np.abs(x - y))\n",
    "    rmse = np.sqrt(np.mean((x - y)**2))\n",
    "    return {\"corr\": float(corr), \"MAE\": float(mae), \"RMSE\": float(rmse)}\n",
    "\n",
    "# =========================================================\n",
    "# Heat3D runner (one model)\n",
    "# =========================================================\n",
    "def run_model_to_heat3d(model_path, imgs_norm, msk_bin, integrated_feats, device):\n",
    "    model = load_beit_backbone(model_path, device)\n",
    "    Z = imgs_norm.shape[0]\n",
    "    heat3d = np.zeros((Z, TARGET_XY[1], TARGET_XY[0]), dtype=np.float32)  # (Z,Y,X)\n",
    "\n",
    "    for z in range(Z):\n",
    "        x1 = imgs_norm[z:z+1].to(device)\n",
    "        m224 = msk_bin[z].astype(np.float32)\n",
    "        grad224 = compute_integrated_gradmap_one_slice(model, x1, integrated_feats)\n",
    "        heat3d[z] = (grad224 * m224).astype(np.float32)\n",
    "\n",
    "    return heat3d\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    nifti_path = os.path.join(CACHE_SLICE_DIR, NIFTI_NAME)\n",
    "    assert os.path.exists(nifti_path), f\"파일 없음: {nifti_path}\"\n",
    "    assert os.path.exists(MASK_PATH),  f\"마스크 없음: {MASK_PATH}\"\n",
    "\n",
    "    # 모델 경로 구성 + 존재 확인\n",
    "    model_paths = {}\n",
    "    for r in RUNS:\n",
    "        p = os.path.join(MODEL_DIR, MODEL_PATTERN.format(run=r))\n",
    "        assert os.path.exists(p), f\"모델 없음: {p}\"\n",
    "        model_paths[r] = p\n",
    "\n",
    "    # load image/mask\n",
    "    sitk_img, arr = load_nifti_as_zyx(nifti_path)\n",
    "    sitk_msk, msk = load_nifti_as_zyx(MASK_PATH)\n",
    "\n",
    "    # resample to 224\n",
    "    if (sitk_img.GetSize()[0], sitk_img.GetSize()[1]) != TARGET_XY:\n",
    "        sitk_img = resample_xy_keep_z(sitk_img, TARGET_XY, sitk.sitkLinear)\n",
    "        arr = sitk.GetArrayFromImage(sitk_img).astype(np.float32)\n",
    "\n",
    "    if (sitk_msk.GetSize()[0], sitk_msk.GetSize()[1]) != TARGET_XY:\n",
    "        sitk_msk = resample_xy_keep_z(sitk_msk, TARGET_XY, sitk.sitkNearestNeighbor)\n",
    "        msk = sitk.GetArrayFromImage(sitk_msk).astype(np.float32)\n",
    "\n",
    "    assert arr.shape == msk.shape, f\"shape mismatch: {arr.shape} vs {msk.shape}\"\n",
    "\n",
    "    imgs_norm, imgs_vis = volume_to_tensors(arr)\n",
    "    Z = imgs_norm.shape[0]\n",
    "    msk_bin = mask_to_binary(msk, 0.5)\n",
    "    roi = (msk_bin > 0)\n",
    "    spacing_xyz = sitk_img.GetSpacing()  # (X,Y,Z)\n",
    "\n",
    "    base = os.path.splitext(os.path.splitext(NIFTI_NAME)[0])[0]\n",
    "\n",
    "    hot_tag = \"_\".join([str(p) for p in HOT_P_LIST])\n",
    "    out_dir = os.path.join(\n",
    "        OUT_DIR,\n",
    "        f\"{base}_RUNS0_29_HOTP{hot_tag}_run{RUN_TARGET}_specific_freqLE{FREQ_MAX}\"\n",
    "    )\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"[OUT]\", out_dir)\n",
    "\n",
    "    # save MRI masked\n",
    "    if SAVE_MRI_MASKED_NIFTI:\n",
    "        mri3d_masked = (imgs_vis.numpy() * msk_bin.astype(np.float32)).astype(np.float32)\n",
    "        save_zyx_as_nifti(mri3d_masked, sitk_img, os.path.join(out_dir, f\"{base}_mri_masked_224.nii.gz\"))\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Allocate storage for 30 heats\n",
    "    # -----------------------------------------------------\n",
    "    if USE_MEMMAP:\n",
    "        mm_path = os.path.join(out_dir, f\"{base}_heat30runs_float32.dat\")\n",
    "        heats = np.memmap(\n",
    "            mm_path, dtype=\"float32\", mode=\"w+\",\n",
    "            shape=(len(RUNS), Z, TARGET_XY[1], TARGET_XY[0])\n",
    "        )\n",
    "        print(\"[MEMMAP]\", mm_path)\n",
    "    else:\n",
    "        heats = np.zeros((len(RUNS), Z, TARGET_XY[1], TARGET_XY[0]), dtype=np.float32)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1) Compute heatmaps for all runs\n",
    "    # -----------------------------------------------------\n",
    "    for i, r in enumerate(tqdm(RUNS, desc=\"Compute heatmaps (30 runs)\")):\n",
    "        H = run_model_to_heat3d(model_paths[r], imgs_norm, msk_bin, INTEGRATED_FEATS, DEVICE)\n",
    "        heats[i] = H\n",
    "        if SAVE_ALL_RUN_HEAT_NIFTI:\n",
    "            save_zyx_as_nifti(H, sitk_img, os.path.join(out_dir, f\"{base}_heat_integrated_raw_run{r}.nii.gz\"))\n",
    "\n",
    "    if USE_MEMMAP:\n",
    "        heats.flush()\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 2) Consensus heat (공통) 저장\n",
    "    # -----------------------------------------------------\n",
    "    consensus = heats.mean(axis=0).astype(np.float32)  # (Z,Y,X)\n",
    "    save_zyx_as_nifti(consensus, sitk_img, os.path.join(out_dir, f\"{base}_CONSENSUS_mean_raw.nii.gz\"))\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 2.5) run별 hotspot threshold를 p별로 저장할 dict\n",
    "    # run_thr[p][run] = thr\n",
    "    # -----------------------------------------------------\n",
    "    run_thr = {p: {} for p in HOT_P_LIST}\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 3~4) 퍼센타일별로 반복\n",
    "    # -----------------------------------------------------\n",
    "    for HOT_P in HOT_P_LIST:\n",
    "        print(f\"\\n===== HOT_P = {HOT_P} =====\")\n",
    "\n",
    "        # 3-1) hotspot frequency map\n",
    "        freq = np.zeros((Z, TARGET_XY[1], TARGET_XY[0]), dtype=np.uint16)\n",
    "\n",
    "        for i, r in enumerate(tqdm(RUNS, desc=f\"Hotspot freq (p{HOT_P})\")):\n",
    "            M, thr = hotspot_mask(heats[i], p=HOT_P)\n",
    "            run_thr[HOT_P][r] = thr\n",
    "            freq += M.astype(np.uint16)\n",
    "\n",
    "        save_zyx_as_nifti(\n",
    "            freq.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_HOT_FREQ_p{HOT_P}_count.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 3-2) consensus hotspot (비교 기준)\n",
    "        Mc, thr_c = hotspot_mask(consensus, p=HOT_P)\n",
    "        save_zyx_as_nifti(\n",
    "            Mc.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_CONSENSUS_HOT_p{HOT_P}.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 3-3) run_target hotspot & unique\n",
    "        idx_t = RUNS.index(RUN_TARGET)\n",
    "        Ht = heats[idx_t].astype(np.float32)\n",
    "        Mt, thr_t = hotspot_mask(Ht, p=HOT_P)\n",
    "\n",
    "        # 다른 run에서의 빈도만 보려면 self 제외\n",
    "        freq_minus_self = freq.astype(np.int32) - Mt.astype(np.int32)\n",
    "        uniq = (Mt > 0) & (freq_minus_self <= FREQ_MAX)\n",
    "\n",
    "        save_zyx_as_nifti(\n",
    "            Mt.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_run{RUN_TARGET}_HOT_p{HOT_P}.nii.gz\")\n",
    "        )\n",
    "        save_zyx_as_nifti(\n",
    "            uniq.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_run{RUN_TARGET}_UNIQUE_p{HOT_P}_freqLE{FREQ_MAX}.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 특이부위 연속 heat: run_target heat에서 uniq만 남김\n",
    "        Ht_unique = (Ht * uniq.astype(np.float32)).astype(np.float32)\n",
    "        save_zyx_as_nifti(\n",
    "            Ht_unique,\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_run{RUN_TARGET}_UNIQUE_heat_raw_p{HOT_P}.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 4) Table: each run vs consensus (p별 CSV)\n",
    "        rows = []\n",
    "        for i, r in enumerate(tqdm(RUNS, desc=f\"Metrics (run vs consensus) p{HOT_P}\")):\n",
    "            Hr = heats[i].astype(np.float32)\n",
    "            Mr, _ = hotspot_mask(Hr, p=HOT_P)\n",
    "\n",
    "            bm = bin_metrics(Mr, Mc)\n",
    "            cd = centroid_distance(Mr > 0, Mc > 0, spacing_xyz=spacing_xyz)\n",
    "            cm = cont_metrics(Hr, consensus, roi_mask=roi)\n",
    "\n",
    "            # 각 run hotspot이 \"run_target unique\"를 얼마나 포함?\n",
    "            uniq_overlap = float(((Mr > 0) & (uniq > 0)).sum() / ((Mr > 0).sum() + 1e-8))\n",
    "\n",
    "            rows.append({\n",
    "                \"run\": r,\n",
    "                \"hot_p\": HOT_P,\n",
    "                \"thr_run\": run_thr[HOT_P][r],\n",
    "                \"thr_consensus\": thr_c,\n",
    "                \"dice_vs_cons\": bm[\"dice\"],\n",
    "                \"iou_vs_cons\": bm[\"iou\"],\n",
    "                \"prec_vs_cons\": bm[\"precision\"],\n",
    "                \"recall_vs_cons\": bm[\"recall\"],\n",
    "                \"f1_vs_cons\": bm[\"f1\"],\n",
    "                \"centroid_dist_mm_vs_cons\": cd,\n",
    "                \"corr_roi_vs_cons\": cm[\"corr\"],\n",
    "                \"MAE_roi_vs_cons\": cm[\"MAE\"],\n",
    "                \"RMSE_roi_vs_cons\": cm[\"RMSE\"],\n",
    "                \"hot_vox_run\": bm[\"A_vox\"],\n",
    "                \"hot_vox_cons\": bm[\"B_vox\"],\n",
    "                \"uniq_overlap_ratio_with_runTargetUnique\": uniq_overlap,\n",
    "            })\n",
    "\n",
    "        csv_path = os.path.join(out_dir, f\"{base}_metrics_30runs_vs_consensus_p{HOT_P}.csv\")\n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            w.writeheader()\n",
    "            w.writerows(rows)\n",
    "\n",
    "        print(f\"[SAVE] CSV: {csv_path}\")\n",
    "\n",
    "    print(\"\\n✅ DONE (all percentiles)\")\n",
    "    print(\"Output folder:\", out_dir)\n",
    "    print(\"\\n[Key outputs]\")\n",
    "    print(\" - MRI:\", f\"{base}_mri_masked_224.nii.gz\")\n",
    "    print(\" - consensus heat:\", f\"{base}_CONSENSUS_mean_raw.nii.gz\")\n",
    "    for p in HOT_P_LIST:\n",
    "        print(f\" - hotspot freq p{p}:\", f\"{base}_HOT_FREQ_p{p}_count.nii.gz\")\n",
    "        print(f\" - consensus hot p{p}:\", f\"{base}_CONSENSUS_HOT_p{p}.nii.gz\")\n",
    "        print(f\" - run{RUN_TARGET} unique p{p}:\", f\"{base}_run{RUN_TARGET}_UNIQUE_p{p}_freqLE{FREQ_MAX}.nii.gz\")\n",
    "        print(f\" - run{RUN_TARGET} unique heat p{p}:\", f\"{base}_run{RUN_TARGET}_UNIQUE_heat_raw_p{p}.nii.gz\")\n",
    "        print(f\" - CSV p{p}:\", f\"{base}_metrics_30runs_vs_consensus_p{p}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "CACHE_SLICE_DIR = \"./cache_slice\"\n",
    "NIFTI_NAME = \"female_pelvis_t1.nii.gz\"\n",
    "MASK_PATH  = \"./output_mask/female_pelvis_t1.nii.gz\"\n",
    "\n",
    "# 모델 폴더 + 패턴 (run0~run29)\n",
    "MODEL_DIR = \"/Users/oyeoncho/Dropbox/working_directory/prep_cx_ca_image/image_data/models_extract/beit0\"\n",
    "MODEL_PATTERN = \"best_model_beit_run{run}.pt\"\n",
    "RUNS = list(range(30))  # run0~run29 (총 30개)\n",
    "\n",
    "OUT_DIR = \"./vis_out_3d_compare_30runs\"\n",
    "TARGET_XY = (224, 224)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "INTEGRATED_FEATS = [436, 519]\n",
    "\n",
    "# ✅ 여러 hotspot percentile\n",
    "HOT_P_LIST = [50, 70, 90]  # 원하면 [90,95,97,99] 등으로 바꾸면 됨\n",
    "\n",
    "# run4 특이부위: run4 hotspot인데, 다른 run들에서 hotspot 빈도가 <= FREQ_MAX 인 voxel\n",
    "RUN_TARGET = 3\n",
    "FREQ_MAX = 2  # 더 엄격하게: 0~1, 덜 엄격하게: 3~5\n",
    "\n",
    "# 저장 옵션\n",
    "SAVE_MRI_MASKED_NIFTI = True\n",
    "SAVE_ALL_RUN_HEAT_NIFTI = False  # True면 run0~29 heatmap도 전부 NIfTI로 저장(용량 큼)\n",
    "\n",
    "# 메모리 절약: 모든 run heat를 디스크 memmap으로 저장\n",
    "USE_MEMMAP = True\n",
    "\n",
    "# =========================================================\n",
    "# MODEL\n",
    "# =========================================================\n",
    "class BEiTBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"beit_base_patch16_224\", pretrained=False, num_classes=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "def load_beit_backbone(model_path: str, device):\n",
    "    model = BEiTBackbone().to(device)\n",
    "    try:\n",
    "        with torch.serialization.safe_globals([np._core.multiarray.scalar]):\n",
    "            ckpt = torch.load(model_path, map_location=device)\n",
    "    except Exception:\n",
    "        ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "    state_dict = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "    filtered = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"backbone.\")}\n",
    "    if len(filtered) == 0:\n",
    "        filtered = state_dict\n",
    "\n",
    "    model.backbone.load_state_dict(filtered, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# NIfTI I/O\n",
    "# =========================================================\n",
    "def load_nifti_as_zyx(nifti_path: str):\n",
    "    img = sitk.ReadImage(nifti_path)\n",
    "    arr = sitk.GetArrayFromImage(img).astype(np.float32)  # (Z, Y, X)\n",
    "    return img, arr\n",
    "\n",
    "def resample_xy_keep_z(sitk_img, target_xy=(224,224), interp=sitk.sitkLinear):\n",
    "    size = list(sitk_img.GetSize())     # (X, Y, Z)\n",
    "    spacing = list(sitk_img.GetSpacing())\n",
    "\n",
    "    new_size = [int(target_xy[0]), int(target_xy[1]), int(size[2])]\n",
    "    new_spacing = [\n",
    "        spacing[0] * (size[0] / new_size[0]),\n",
    "        spacing[1] * (size[1] / new_size[1]),\n",
    "        spacing[2]\n",
    "    ]\n",
    "\n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetSize(new_size)\n",
    "    resampler.SetOutputSpacing(new_spacing)\n",
    "    resampler.SetOutputOrigin(sitk_img.GetOrigin())\n",
    "    resampler.SetOutputDirection(sitk_img.GetDirection())\n",
    "    resampler.SetInterpolator(interp)\n",
    "    return resampler.Execute(sitk_img)\n",
    "\n",
    "def save_zyx_as_nifti(vol_zyx: np.ndarray, ref_img_sitk, out_path: str):\n",
    "    out = sitk.GetImageFromArray(vol_zyx.astype(np.float32))  # expects (Z,Y,X)\n",
    "    out.SetSpacing(ref_img_sitk.GetSpacing())\n",
    "    out.SetOrigin(ref_img_sitk.GetOrigin())\n",
    "    out.SetDirection(ref_img_sitk.GetDirection())\n",
    "    sitk.WriteImage(out, out_path)\n",
    "\n",
    "# =========================================================\n",
    "# Preprocess\n",
    "# =========================================================\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1,3,1,1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1,3,1,1)\n",
    "\n",
    "def volume_to_tensors(arr_zyx: np.ndarray):\n",
    "    t = torch.from_numpy(arr_zyx).float()\n",
    "    vmin = float(t.min().item()); vmax = float(t.max().item())\n",
    "    if vmax > vmin:\n",
    "        t01 = (t - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        t01 = torch.zeros_like(t)\n",
    "\n",
    "    imgs_vis = t01.clone()                          # (Z,Y,X) in [0,1]\n",
    "    x = t01.unsqueeze(1).repeat(1, 3, 1, 1)         # (Z,3,Y,X)\n",
    "    x = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return x, imgs_vis\n",
    "\n",
    "def mask_to_binary(mask_zyx: np.ndarray, thr=0.5):\n",
    "    return (mask_zyx > thr).astype(np.uint8)\n",
    "\n",
    "def upsample_14_to_224(m14: torch.Tensor):\n",
    "    m = m14.unsqueeze(0).unsqueeze(0)  # (1,1,14,14)\n",
    "    up = F.interpolate(m, size=(224,224), mode=\"bilinear\", align_corners=False)[0,0]\n",
    "    return up\n",
    "\n",
    "# =========================================================\n",
    "# Integrated grad heatmap (raw) for one slice\n",
    "# =========================================================\n",
    "def compute_integrated_gradmap_one_slice(model: BEiTBackbone, x1: torch.Tensor, integrated_feats):\n",
    "    tokens = model.backbone.forward_features(x1)  # (1,1+N,C)\n",
    "    tokens.retain_grad()\n",
    "\n",
    "    if hasattr(model.backbone, \"forward_head\"):\n",
    "        feats = model.backbone.forward_head(tokens, pre_logits=True)  # (1,C)\n",
    "    else:\n",
    "        feats = tokens[:, 0]\n",
    "\n",
    "    patch_tok = tokens[:, 1:, :]  # (1,N,C)\n",
    "    n = patch_tok.shape[1]\n",
    "    h = w = int(np.sqrt(n))\n",
    "\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    if tokens.grad is not None:\n",
    "        tokens.grad.zero_()\n",
    "\n",
    "    target = sum(feats[:, idx].sum() for idx in integrated_feats)\n",
    "    target.backward(retain_graph=False)\n",
    "\n",
    "    patch_grad = tokens.grad[:, 1:, :]\n",
    "    imp14 = (patch_grad[0] * patch_tok[0]).abs().mean(dim=-1).reshape(h, w)  # (14,14)\n",
    "    imp224 = upsample_14_to_224(imp14).detach().cpu().numpy().astype(np.float32)\n",
    "    return imp224\n",
    "\n",
    "# =========================================================\n",
    "# Hotspot + metrics\n",
    "# =========================================================\n",
    "def hotspot_mask(H, p=95):\n",
    "    pos = H[H > 0]\n",
    "    if pos.size < 10:\n",
    "        return np.zeros_like(H, dtype=np.uint8), np.nan\n",
    "    thr = np.percentile(pos, p)\n",
    "    return (H >= thr).astype(np.uint8), float(thr)\n",
    "\n",
    "def bin_metrics(A, B):\n",
    "    A = (A > 0); B = (B > 0)\n",
    "    inter = np.logical_and(A, B).sum()\n",
    "    union = np.logical_or(A, B).sum()\n",
    "    a = A.sum(); b = B.sum()\n",
    "\n",
    "    dice = (2*inter) / (a + b + 1e-8)\n",
    "    iou  = inter / (union + 1e-8)\n",
    "\n",
    "    tp = inter\n",
    "    fp = np.logical_and(A, ~B).sum()\n",
    "    fn = np.logical_and(~A, B).sum()\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "    f1        = 2*precision*recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"A_vox\": int(a), \"B_vox\": int(b),\n",
    "        \"inter\": int(inter), \"union\": int(union),\n",
    "        \"dice\": float(dice), \"iou\": float(iou),\n",
    "        \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1),\n",
    "        \"fp\": int(fp), \"fn\": int(fn),\n",
    "    }\n",
    "\n",
    "def centroid_distance(A, B, spacing_xyz=None):\n",
    "    A_idx = np.argwhere(A)\n",
    "    B_idx = np.argwhere(B)\n",
    "    if A_idx.size == 0 or B_idx.size == 0:\n",
    "        return np.nan\n",
    "    ca = A_idx.mean(axis=0)  # (Z,Y,X)\n",
    "    cb = B_idx.mean(axis=0)\n",
    "    d = ca - cb\n",
    "    if spacing_xyz is not None:\n",
    "        sx, sy, sz = spacing_xyz  # (X,Y,Z)\n",
    "        d = np.array([d[0]*sz, d[1]*sy, d[2]*sx], dtype=np.float32)\n",
    "    return float(np.linalg.norm(d))\n",
    "\n",
    "def cont_metrics(H1, H2, roi_mask):\n",
    "    x = H1[roi_mask].astype(np.float64)\n",
    "    y = H2[roi_mask].astype(np.float64)\n",
    "\n",
    "    x0 = x - x.mean()\n",
    "    y0 = y - y.mean()\n",
    "    corr = (x0*y0).sum() / (np.sqrt((x0*x0).sum() * (y0*y0).sum()) + 1e-12)\n",
    "\n",
    "    mae  = np.mean(np.abs(x - y))\n",
    "    rmse = np.sqrt(np.mean((x - y)**2))\n",
    "    return {\"corr\": float(corr), \"MAE\": float(mae), \"RMSE\": float(rmse)}\n",
    "\n",
    "# =========================================================\n",
    "# Heat3D runner (one model)\n",
    "# =========================================================\n",
    "def run_model_to_heat3d(model_path, imgs_norm, msk_bin, integrated_feats, device):\n",
    "    model = load_beit_backbone(model_path, device)\n",
    "    Z = imgs_norm.shape[0]\n",
    "    heat3d = np.zeros((Z, TARGET_XY[1], TARGET_XY[0]), dtype=np.float32)  # (Z,Y,X)\n",
    "\n",
    "    for z in range(Z):\n",
    "        x1 = imgs_norm[z:z+1].to(device)\n",
    "        m224 = msk_bin[z].astype(np.float32)\n",
    "        grad224 = compute_integrated_gradmap_one_slice(model, x1, integrated_feats)\n",
    "        heat3d[z] = (grad224 * m224).astype(np.float32)\n",
    "\n",
    "    return heat3d\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    nifti_path = os.path.join(CACHE_SLICE_DIR, NIFTI_NAME)\n",
    "    assert os.path.exists(nifti_path), f\"파일 없음: {nifti_path}\"\n",
    "    assert os.path.exists(MASK_PATH),  f\"마스크 없음: {MASK_PATH}\"\n",
    "\n",
    "    # 모델 경로 구성 + 존재 확인\n",
    "    model_paths = {}\n",
    "    for r in RUNS:\n",
    "        p = os.path.join(MODEL_DIR, MODEL_PATTERN.format(run=r))\n",
    "        assert os.path.exists(p), f\"모델 없음: {p}\"\n",
    "        model_paths[r] = p\n",
    "\n",
    "    # load image/mask\n",
    "    sitk_img, arr = load_nifti_as_zyx(nifti_path)\n",
    "    sitk_msk, msk = load_nifti_as_zyx(MASK_PATH)\n",
    "\n",
    "    # resample to 224\n",
    "    if (sitk_img.GetSize()[0], sitk_img.GetSize()[1]) != TARGET_XY:\n",
    "        sitk_img = resample_xy_keep_z(sitk_img, TARGET_XY, sitk.sitkLinear)\n",
    "        arr = sitk.GetArrayFromImage(sitk_img).astype(np.float32)\n",
    "\n",
    "    if (sitk_msk.GetSize()[0], sitk_msk.GetSize()[1]) != TARGET_XY:\n",
    "        sitk_msk = resample_xy_keep_z(sitk_msk, TARGET_XY, sitk.sitkNearestNeighbor)\n",
    "        msk = sitk.GetArrayFromImage(sitk_msk).astype(np.float32)\n",
    "\n",
    "    assert arr.shape == msk.shape, f\"shape mismatch: {arr.shape} vs {msk.shape}\"\n",
    "\n",
    "    imgs_norm, imgs_vis = volume_to_tensors(arr)\n",
    "    Z = imgs_norm.shape[0]\n",
    "    msk_bin = mask_to_binary(msk, 0.5)\n",
    "    roi = (msk_bin > 0)\n",
    "    spacing_xyz = sitk_img.GetSpacing()  # (X,Y,Z)\n",
    "\n",
    "    base = os.path.splitext(os.path.splitext(NIFTI_NAME)[0])[0]\n",
    "\n",
    "    hot_tag = \"_\".join([str(p) for p in HOT_P_LIST])\n",
    "    out_dir = os.path.join(\n",
    "        OUT_DIR,\n",
    "        f\"{base}_RUNS0_29_HOTP{hot_tag}_run{RUN_TARGET}_specific_freqLE{FREQ_MAX}\"\n",
    "    )\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"[OUT]\", out_dir)\n",
    "\n",
    "    # save MRI masked\n",
    "    if SAVE_MRI_MASKED_NIFTI:\n",
    "        mri3d_masked = (imgs_vis.numpy() * msk_bin.astype(np.float32)).astype(np.float32)\n",
    "        save_zyx_as_nifti(mri3d_masked, sitk_img, os.path.join(out_dir, f\"{base}_mri_masked_224.nii.gz\"))\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Allocate storage for 30 heats\n",
    "    # -----------------------------------------------------\n",
    "    if USE_MEMMAP:\n",
    "        mm_path = os.path.join(out_dir, f\"{base}_heat30runs_float32.dat\")\n",
    "        heats = np.memmap(\n",
    "            mm_path, dtype=\"float32\", mode=\"w+\",\n",
    "            shape=(len(RUNS), Z, TARGET_XY[1], TARGET_XY[0])\n",
    "        )\n",
    "        print(\"[MEMMAP]\", mm_path)\n",
    "    else:\n",
    "        heats = np.zeros((len(RUNS), Z, TARGET_XY[1], TARGET_XY[0]), dtype=np.float32)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1) Compute heatmaps for all runs\n",
    "    # -----------------------------------------------------\n",
    "    for i, r in enumerate(tqdm(RUNS, desc=\"Compute heatmaps (30 runs)\")):\n",
    "        H = run_model_to_heat3d(model_paths[r], imgs_norm, msk_bin, INTEGRATED_FEATS, DEVICE)\n",
    "        heats[i] = H\n",
    "        if SAVE_ALL_RUN_HEAT_NIFTI:\n",
    "            save_zyx_as_nifti(H, sitk_img, os.path.join(out_dir, f\"{base}_heat_integrated_raw_run{r}.nii.gz\"))\n",
    "\n",
    "    if USE_MEMMAP:\n",
    "        heats.flush()\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 2) Consensus heat (공통) 저장\n",
    "    # -----------------------------------------------------\n",
    "    consensus = heats.mean(axis=0).astype(np.float32)  # (Z,Y,X)\n",
    "    save_zyx_as_nifti(consensus, sitk_img, os.path.join(out_dir, f\"{base}_CONSENSUS_mean_raw.nii.gz\"))\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 2.5) run별 hotspot threshold를 p별로 저장할 dict\n",
    "    # run_thr[p][run] = thr\n",
    "    # -----------------------------------------------------\n",
    "    run_thr = {p: {} for p in HOT_P_LIST}\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 3~4) 퍼센타일별로 반복\n",
    "    # -----------------------------------------------------\n",
    "    for HOT_P in HOT_P_LIST:\n",
    "        print(f\"\\n===== HOT_P = {HOT_P} =====\")\n",
    "\n",
    "        # 3-1) hotspot frequency map\n",
    "        freq = np.zeros((Z, TARGET_XY[1], TARGET_XY[0]), dtype=np.uint16)\n",
    "\n",
    "        for i, r in enumerate(tqdm(RUNS, desc=f\"Hotspot freq (p{HOT_P})\")):\n",
    "            M, thr = hotspot_mask(heats[i], p=HOT_P)\n",
    "            run_thr[HOT_P][r] = thr\n",
    "            freq += M.astype(np.uint16)\n",
    "\n",
    "        save_zyx_as_nifti(\n",
    "            freq.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_HOT_FREQ_p{HOT_P}_count.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 3-2) consensus hotspot (비교 기준)\n",
    "        Mc, thr_c = hotspot_mask(consensus, p=HOT_P)\n",
    "        save_zyx_as_nifti(\n",
    "            Mc.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_CONSENSUS_HOT_p{HOT_P}.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 3-3) run_target hotspot & unique\n",
    "        idx_t = RUNS.index(RUN_TARGET)\n",
    "        Ht = heats[idx_t].astype(np.float32)\n",
    "        Mt, thr_t = hotspot_mask(Ht, p=HOT_P)\n",
    "\n",
    "        # 다른 run에서의 빈도만 보려면 self 제외\n",
    "        freq_minus_self = freq.astype(np.int32) - Mt.astype(np.int32)\n",
    "        uniq = (Mt > 0) & (freq_minus_self <= FREQ_MAX)\n",
    "\n",
    "        save_zyx_as_nifti(\n",
    "            Mt.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_run{RUN_TARGET}_HOT_p{HOT_P}.nii.gz\")\n",
    "        )\n",
    "        save_zyx_as_nifti(\n",
    "            uniq.astype(np.float32),\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_run{RUN_TARGET}_UNIQUE_p{HOT_P}_freqLE{FREQ_MAX}.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 특이부위 연속 heat: run_target heat에서 uniq만 남김\n",
    "        Ht_unique = (Ht * uniq.astype(np.float32)).astype(np.float32)\n",
    "        save_zyx_as_nifti(\n",
    "            Ht_unique,\n",
    "            sitk_img,\n",
    "            os.path.join(out_dir, f\"{base}_run{RUN_TARGET}_UNIQUE_heat_raw_p{HOT_P}.nii.gz\")\n",
    "        )\n",
    "\n",
    "        # 4) Table: each run vs consensus (p별 CSV)\n",
    "        rows = []\n",
    "        for i, r in enumerate(tqdm(RUNS, desc=f\"Metrics (run vs consensus) p{HOT_P}\")):\n",
    "            Hr = heats[i].astype(np.float32)\n",
    "            Mr, _ = hotspot_mask(Hr, p=HOT_P)\n",
    "\n",
    "            bm = bin_metrics(Mr, Mc)\n",
    "            cd = centroid_distance(Mr > 0, Mc > 0, spacing_xyz=spacing_xyz)\n",
    "            cm = cont_metrics(Hr, consensus, roi_mask=roi)\n",
    "\n",
    "            # 각 run hotspot이 \"run_target unique\"를 얼마나 포함?\n",
    "            uniq_overlap = float(((Mr > 0) & (uniq > 0)).sum() / ((Mr > 0).sum() + 1e-8))\n",
    "\n",
    "            rows.append({\n",
    "                \"run\": r,\n",
    "                \"hot_p\": HOT_P,\n",
    "                \"thr_run\": run_thr[HOT_P][r],\n",
    "                \"thr_consensus\": thr_c,\n",
    "                \"dice_vs_cons\": bm[\"dice\"],\n",
    "                \"iou_vs_cons\": bm[\"iou\"],\n",
    "                \"prec_vs_cons\": bm[\"precision\"],\n",
    "                \"recall_vs_cons\": bm[\"recall\"],\n",
    "                \"f1_vs_cons\": bm[\"f1\"],\n",
    "                \"centroid_dist_mm_vs_cons\": cd,\n",
    "                \"corr_roi_vs_cons\": cm[\"corr\"],\n",
    "                \"MAE_roi_vs_cons\": cm[\"MAE\"],\n",
    "                \"RMSE_roi_vs_cons\": cm[\"RMSE\"],\n",
    "                \"hot_vox_run\": bm[\"A_vox\"],\n",
    "                \"hot_vox_cons\": bm[\"B_vox\"],\n",
    "                \"uniq_overlap_ratio_with_runTargetUnique\": uniq_overlap,\n",
    "            })\n",
    "\n",
    "        csv_path = os.path.join(out_dir, f\"{base}_metrics_30runs_vs_consensus_p{HOT_P}.csv\")\n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            w.writeheader()\n",
    "            w.writerows(rows)\n",
    "\n",
    "        print(f\"[SAVE] CSV: {csv_path}\")\n",
    "\n",
    "    print(\"\\n✅ DONE (all percentiles)\")\n",
    "    print(\"Output folder:\", out_dir)\n",
    "    print(\"\\n[Key outputs]\")\n",
    "    print(\" - MRI:\", f\"{base}_mri_masked_224.nii.gz\")\n",
    "    print(\" - consensus heat:\", f\"{base}_CONSENSUS_mean_raw.nii.gz\")\n",
    "    for p in HOT_P_LIST:\n",
    "        print(f\" - hotspot freq p{p}:\", f\"{base}_HOT_FREQ_p{p}_count.nii.gz\")\n",
    "        print(f\" - consensus hot p{p}:\", f\"{base}_CONSENSUS_HOT_p{p}.nii.gz\")\n",
    "        print(f\" - run{RUN_TARGET} unique p{p}:\", f\"{base}_run{RUN_TARGET}_UNIQUE_p{p}_freqLE{FREQ_MAX}.nii.gz\")\n",
    "        print(f\" - run{RUN_TARGET} unique heat p{p}:\", f\"{base}_run{RUN_TARGET}_UNIQUE_heat_raw_p{p}.nii.gz\")\n",
    "        print(f\" - CSV p{p}:\", f\"{base}_metrics_30runs_vs_consensus_p{p}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
