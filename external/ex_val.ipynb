{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External validation (n7)\n",
    "# Mixture Stretched Exponential Survival with Debugging, NaN Handling, Model Saving, and External Evaluation\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lifelines.utils import concordance_index\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "# =========================\n",
    "# Config (ìƒë‹¨ ì„¤ì • ì¼ë¶€ë§Œ ìˆ˜ì •)\n",
    "# =========================\n",
    "BASE_GROUPS = [\"beit0\"]\n",
    "GROUP = \"n7_30_30\"\n",
    "N_RUNS = 30\n",
    "time_points = [12, 24, 36, 48, 60, 72]\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# âœ… ì „ì—­ ì¬í˜„ì„± ê³ ì •ìš© ê¸°ë³¸ ì‹œë“œ\n",
    "BASE_SEED = 20250903\n",
    "\n",
    "# ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "os.makedirs(\"./survival_model/mixture_non_fix/models\", exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "\n",
    "def evaluate_external_for_all_models(MODEL_DIR, EXTERNAL_CSV, time_points, device=DEVICE):\n",
    "    if not os.path.exists(EXTERNAL_CSV):\n",
    "        print(f\"â„¹ï¸ ì™¸ë¶€ í‰ê°€ ìŠ¤í‚µ (íŒŒì¼ ì—†ìŒ): {EXTERNAL_CSV}\")\n",
    "        return None, None\n",
    "\n",
    "    df_ext = pd.read_csv(EXTERNAL_CSV)\n",
    "\n",
    "    # ì™¸ë¶€ ë°ì´í„° event/time ì»¬ëŸ¼ ë³´ì •\n",
    "    if 'event' not in df_ext.columns and 'survival' in df_ext.columns:\n",
    "        df_ext['event'] = df_ext['survival'].astype(int)\n",
    "    if 'time' not in df_ext.columns and 'fu_date' in df_ext.columns:\n",
    "        df_ext['time'] = df_ext['fu_date'].astype(np.float32)\n",
    "\n",
    "    # MODEL_DIR ë‚´ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ìŠ¤ìº” (ì˜ˆ: best_model_run13_Image_only.pt)\n",
    "    ckpt_paths = glob.glob(os.path.join(MODEL_DIR, \"best_model_run*.pt\"))\n",
    "    if not ckpt_paths:\n",
    "        print(f\"âš ï¸ {MODEL_DIR} ì— ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "\n",
    "    pattern = re.compile(r\"best_model_run(\\d+)_([^\\.]+)\\.pt\")\n",
    "    ext_rows_auc_all, ext_rows_cidx_all = [], []\n",
    "\n",
    "    for ckpt_path in sorted(ckpt_paths):\n",
    "        m = pattern.search(os.path.basename(ckpt_path))\n",
    "        if not m:\n",
    "            print(f\"âš ï¸ ìŠ¤í‚µ(íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨): {ckpt_path}\")\n",
    "            continue\n",
    "        run_idx = int(m.group(1))\n",
    "        label_raw = m.group(2)                  # e.g., Image_only\n",
    "        label = label_raw.replace('_', ' ')     # \"Image only\"\n",
    "\n",
    "        # ColumnTransformer ê²½ë¡œ\n",
    "        ct_path = os.path.join(MODEL_DIR, f\"ct_run{run_idx:02d}_{label_raw}.joblib\")\n",
    "        if not os.path.exists(ct_path):\n",
    "            print(f\"âš ï¸ ì „ì²˜ë¦¬ê¸° ëˆ„ë½ â†’ ìŠ¤í‚µ: {ct_path}\")\n",
    "            continue\n",
    "\n",
    "        # ëª¨ë¸/CT ë¡œë“œ\n",
    "        try:\n",
    "            model, ct = load_model_and_ct(MODEL_DIR, run_idx, label, device=device, num_components=2)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ: run{run_idx:02d}, {label}\")\n",
    "            continue\n",
    "\n",
    "        # ColumnTransformerê°€ ê¸°ëŒ€í•˜ëŠ” **ì›ë³¸ ì…ë ¥ ì»¬ëŸ¼** ëª¨ìœ¼ê¸°\n",
    "        required_columns = []\n",
    "        for name, trans, cols in ct.transformers_:\n",
    "            if cols is None or cols == []:\n",
    "                continue\n",
    "            if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "                required_columns.extend(list(cols))\n",
    "            else:\n",
    "                required_columns.append(cols)\n",
    "\n",
    "        missing = [c for c in required_columns if c not in df_ext.columns]\n",
    "        if missing:\n",
    "            print(f\"âš ï¸ ì™¸ë¶€ ë°ì´í„°ì— '{label}' ì…ë ¥ ì»¬ëŸ¼ ëˆ„ë½: {missing} â†’ ìŠ¤í‚µ(run{run_idx:02d})\")\n",
    "            continue\n",
    "\n",
    "        # ë³€í™˜ ë° ì˜ˆì¸¡\n",
    "        X_ext = ct.transform(df_ext[required_columns])\n",
    "        X_ext = pd.DataFrame(X_ext).fillna(0).values.astype(np.float32)\n",
    "        X_ext_tensor = torch.tensor(X_ext, dtype=torch.float32).to(device)\n",
    "\n",
    "        surv_ext = predict_survival(model, X_ext_tensor, time_points)  # (T, N)\n",
    "        y_ext = df_ext[['time', 'event']].copy()\n",
    "\n",
    "        # AUC / C-index\n",
    "        auc_ext = calc_auc(surv_ext, y_ext.reset_index(drop=True), time_points)\n",
    "        risk_ext = surv_ext.T\n",
    "        cidx_ext = [safe_concordance_index(y_ext['time'], risk_ext[:, j], y_ext['event'])\n",
    "                    for j in range(len(time_points))]\n",
    "\n",
    "        # ëˆ„ì  ì €ì¥\n",
    "        for j, t in enumerate(time_points):\n",
    "            ext_rows_auc_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                     \"Time (Months)\": t, \"AUC (External)\": auc_ext[t]})\n",
    "            ext_rows_cidx_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                      \"Time (Months)\": t, \"C-index (External)\": cidx_ext[j]})\n",
    "        ext_rows_auc_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                 \"Time (Months)\": \"Overall\",\n",
    "                                 \"AUC (External)\": np.nanmean(list(auc_ext.values()))})\n",
    "        ext_rows_cidx_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                  \"Time (Months)\": \"Overall\",\n",
    "                                  \"C-index (External)\": np.nanmean(cidx_ext)})\n",
    "\n",
    "    return ext_rows_auc_all, ext_rows_cidx_all\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class MixtureStretchedExponentialSurvival(nn.Module):\n",
    "    def __init__(self, input_dim, num_components=2):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU()\n",
    "        )\n",
    "        self.pi_layer = nn.Linear(64, num_components)\n",
    "        self.lam_layer = nn.Linear(64, num_components)\n",
    "        self.alpha_layer = nn.Linear(64, num_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        pi = F.softmax(self.pi_layer(h), dim=1)\n",
    "        lam = F.softplus(self.lam_layer(h)) + 1e-3\n",
    "        a = F.softplus(self.alpha_layer(h)) + 1e-3\n",
    "        return pi, lam, a\n",
    "\n",
    "def mixture_stretched_nll(t, e, pi, lam, a, eps=1e-8):\n",
    "    t = t.view(-1, 1)\n",
    "    t_a = torch.pow(t + eps, a)\n",
    "    S_k = torch.exp(-lam * t_a)\n",
    "    f_k = lam * a * torch.pow(t + eps, a - 1) * S_k\n",
    "    f = torch.sum(pi * f_k, dim=1) + eps\n",
    "    S = torch.sum(pi * S_k, dim=1) + eps\n",
    "    loglik = e * torch.log(f) + (1 - e) * torch.log(S)\n",
    "    return -loglik.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_survival(model, x, times):\n",
    "    model.eval()\n",
    "    pi, lam, a = model(x)\n",
    "    surv = []\n",
    "    for t in times:\n",
    "        t_tensor = torch.tensor([t], dtype=torch.float32, device=x.device)\n",
    "        t_a = torch.pow(t_tensor + 1e-8, a)\n",
    "        S_k = torch.exp(-lam * t_a)\n",
    "        S = torch.sum(pi * S_k, dim=1)\n",
    "        surv.append(S.cpu().numpy())\n",
    "    return np.vstack(surv)  # shape: (len(times), N)\n",
    "\n",
    "def calc_auc(surv_arr, y_df, times):\n",
    "    aucs = {}\n",
    "    for i, t in enumerate(times):\n",
    "        true = ((y_df[\"event\"] == 1) & (y_df[\"time\"] <= t)).astype(int)\n",
    "        pred = 1 - surv_arr[i, :]\n",
    "        try:\n",
    "            aucs[t] = roc_auc_score(true, pred)\n",
    "        except Exception:\n",
    "            aucs[t] = np.nan\n",
    "    return aucs\n",
    "\n",
    "def safe_concordance_index(times, risks, events):\n",
    "    times = np.asarray(times)\n",
    "    risks = np.asarray(risks)\n",
    "    events = np.asarray(events)\n",
    "    mask = ~(np.isnan(times) | np.isnan(risks) | np.isnan(events))\n",
    "    if np.sum(mask) < 2:\n",
    "        print(\"âš ï¸ Too few valid samples for C-index:\", np.sum(mask))\n",
    "        return np.nan\n",
    "    if np.std(risks[mask]) < 1e-6:\n",
    "        print(\"âš ï¸ Low risk variance, skipping C-index\")\n",
    "        return np.nan\n",
    "    return concordance_index(times[mask], risks[mask], events[mask])\n",
    "\n",
    "def save_model_and_ct(model_state, ct, save_dir, run_idx, label, input_dim):\n",
    "    tag = f\"run{run_idx:02d}_{label.replace(' ', '_')}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save({\n",
    "        \"state_dict\": model_state,\n",
    "        \"input_dim\": input_dim\n",
    "    }, os.path.join(save_dir, f\"best_model_{tag}.pt\"))\n",
    "    joblib.dump(ct, os.path.join(save_dir, f\"ct_{tag}.joblib\"))\n",
    "\n",
    "def load_model_and_ct(model_dir, run_idx, label, device=DEVICE, num_components=2):\n",
    "    tag = f\"run{run_idx:02d}_{label.replace(' ', '_')}\"\n",
    "    ckpt_path = os.path.join(model_dir, f\"best_model_{tag}.pt\")\n",
    "    ct_path   = os.path.join(model_dir, f\"ct_{tag}.joblib\")\n",
    "\n",
    "    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë¨¼ì € ì²´í¬\n",
    "    if (not os.path.exists(ckpt_path)) or (not os.path.exists(ct_path)):\n",
    "        raise FileNotFoundError(f\"ëª¨ë¸ ë˜ëŠ” ì „ì²˜ë¦¬ê¸° íŒŒì¼ ì—†ìŒ: {ckpt_path}, {ct_path}\")\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    input_dim = ckpt[\"input_dim\"]\n",
    "    model = MixtureStretchedExponentialSurvival(\n",
    "        input_dim=input_dim,\n",
    "        num_components=num_components\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    # ColumnTransformer ë¡œë“œ (ê¹¨ì§„ íŒŒì¼ ì²˜ë¦¬)\n",
    "    try:\n",
    "        ct = joblib.load(ct_path)\n",
    "    except EOFError as e:\n",
    "        print(f\"âš ï¸ ColumnTransformer íŒŒì¼ ì†ìƒ (EOFError) â†’ ìŠ¤í‚µ: {ct_path}\")\n",
    "        # evaluate_external_for_all_models ìª½ì—ì„œ FileNotFoundErrorë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ,\n",
    "        # ì—¬ê¸°ì„œ FileNotFoundErrorë¡œ ì¬í¬ì¥í•´ì„œ ë˜ì ¸ì¤Œ\n",
    "        raise FileNotFoundError(ct_path) from e\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ColumnTransformer ë¡œë“œ ì˜¤ë¥˜ â†’ ìŠ¤í‚µ: {ct_path} ({e})\")\n",
    "        raise FileNotFoundError(ct_path) from e\n",
    "\n",
    "    model.eval()\n",
    "    return model, ct\n",
    "\n",
    "\n",
    "def evaluate_on_dataframe(model, ct, df, feature_cols, time_points, device=DEVICE):\n",
    "    X_df = df[feature_cols].copy()\n",
    "    X = ct.transform(X_df)\n",
    "    X = pd.DataFrame(X).fillna(0).values.astype(np.float32)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    surv = predict_survival(model, X_tensor, time_points)  # (T, N)\n",
    "    return surv\n",
    "# =========================\n",
    "# Main (ì´ ë¸”ë¡ ì „ì²´ êµì²´)\n",
    "# =========================\n",
    "device = DEVICE\n",
    "\n",
    "for base_group in BASE_GROUPS:\n",
    "    print(f\"\\n\\n============================\")\n",
    "    print(f\"ğŸ“ BEiT ê·¸ë£¹ ì‹¤í–‰ ì¤‘: {base_group}\")\n",
    "    print(f\"============================\")\n",
    "\n",
    "    SAVE_ROOT_BASE = f\"./survival_model/mixture_non_fix/non_nest/{base_group}/results/generalization/test1_1/dl0/{GROUP}\"\n",
    "    MODEL_DIR_ROOT = f\"./survival_model/mixture_non_fix/models/{base_group}/{GROUP}\"\n",
    "    os.makedirs(SAVE_ROOT_BASE, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR_ROOT, exist_ok=True)\n",
    "\n",
    "    # ê³µí†µ ì»¬ëŸ¼ ì„¤ì •\n",
    "    img_cols = [\"feat_436\", \"feat_519\"]\n",
    "    cont_cols = ['Age']\n",
    "    cat_cols  = ['pathology', 'stage0']\n",
    "\n",
    "    feature_sets = {\n",
    "        'Image only': (img_cols, []),\n",
    "        'Clinical only': ([], cont_cols + cat_cols),\n",
    "        'Image + Clinical': (img_cols, cont_cols + cat_cols)\n",
    "    }\n",
    "\n",
    "    # âœ… ONLY_RUN_IDX = 1..30 & EXTERNAL_CSV = ./external/external{idx}.csv ë¡œ ë°˜ë³µ\n",
    "    for i in range(1, 31):\n",
    "        ONLY_RUN_IDX = i\n",
    "        EXTERNAL_CSV = f\"./external/external{ONLY_RUN_IDX}.csv\"\n",
    "\n",
    "        fname = f\"dh11_run{ONLY_RUN_IDX:02d}.csv\"\n",
    "        csv_path = f\"./deephit/{base_group}/test/dl0/{GROUP}/{fname}\"\n",
    "        print(f\"\\nğŸš€ ì‹¤í–‰ ì¤‘: {base_group} - {fname} | EXTERNAL: external{ONLY_RUN_IDX}.csv\")\n",
    "\n",
    "        # âœ… ië³„ í´ë”\n",
    "        MODEL_DIR_I = os.path.join(MODEL_DIR_ROOT, f\"file{ONLY_RUN_IDX:02d}\")\n",
    "        SAVE_ROOT   = os.path.join(SAVE_ROOT_BASE, f\"file{ONLY_RUN_IDX:02d}\")\n",
    "        os.makedirs(MODEL_DIR_I, exist_ok=True)\n",
    "        os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âš ï¸ ë‚´ë¶€ ë°ì´í„° ì—†ìŒ â†’ ìŠ¤í‚µ: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        df_all = pd.read_csv(csv_path)\n",
    "        # event/time ì—´ ë§¤í•‘\n",
    "        if 'event' not in df_all.columns and 'survival' in df_all.columns:\n",
    "            df_all['event'] = df_all['survival'].astype(int)\n",
    "        if 'time' not in df_all.columns and 'fu_date' in df_all.columns:\n",
    "            df_all['time']  = df_all['fu_date'].astype(np.float32)\n",
    "\n",
    "        results_dict = {}\n",
    "        raw_rows_auc, raw_rows_cidx = [], []\n",
    "\n",
    "        for label, (img_part, clinical_part) in feature_sets.items():\n",
    "            print(f\"\\nğŸ“Œ Feature Set: {label}\")\n",
    "            auc_train_list, auc_val_list = [], []\n",
    "            cidx_train_list, cidx_val_list = [], []\n",
    "\n",
    "            # ----- Monte Carlo N_RUNS -----\n",
    "            for run in range(N_RUNS):\n",
    "                # âœ… ì‹œë“œ ê³ ì •: ì „ì—­ ê¸°ë³¸ ì‹œë“œ + run (í•­ìƒ ë™ì¼ ì¬í˜„)\n",
    "                set_seed(BASE_SEED + run)\n",
    "\n",
    "                # Design X / y\n",
    "                used_cols = img_part + clinical_part\n",
    "                X_df = df_all[used_cols].copy()\n",
    "                y_df = df_all[['time', 'event']].copy()\n",
    "\n",
    "                X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "                    X_df, y_df, test_size=0.3, random_state=BASE_SEED + run\n",
    "                )\n",
    "\n",
    "                transformers = []\n",
    "                if img_part:\n",
    "                    transformers.append(('img', StandardScaler(), img_part))\n",
    "                cont = [c for c in clinical_part if c in cont_cols]\n",
    "                cat  = [c for c in clinical_part if c in cat_cols]\n",
    "                if cont:\n",
    "                    transformers.append(('cont', StandardScaler(), cont))\n",
    "                if cat:\n",
    "                    transformers.append(('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat))\n",
    "\n",
    "                ct = ColumnTransformer(transformers)\n",
    "                X_train = ct.fit_transform(X_train_df)\n",
    "                X_val   = ct.transform(X_val_df)\n",
    "\n",
    "                X_train = pd.DataFrame(X_train).fillna(0).values.astype(np.float32)\n",
    "                X_val   = pd.DataFrame(X_val).fillna(0).values.astype(np.float32)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "                X_val_tensor   = torch.tensor(X_val,   dtype=torch.float32).to(device)\n",
    "                t_train = torch.tensor(y_train['time'].values,  dtype=torch.float32).to(device)\n",
    "                e_train = torch.tensor(y_train['event'].values, dtype=torch.float32).to(device)\n",
    "\n",
    "                model = MixtureStretchedExponentialSurvival(\n",
    "                    input_dim=X_train.shape[1], num_components=2\n",
    "                ).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "                best_val_loss = float('inf')\n",
    "                patience, patience_counter = 10, 0\n",
    "                best_model_state = None\n",
    "\n",
    "                for epoch in range(1000):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    pi, lam, a = model(X_train_tensor)\n",
    "                    loss = mixture_stretched_nll(t_train, e_train, pi, lam, a)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # ê°„ë‹¨ Early Stopping (train loss ê¸°ì¤€, ì› ì½”ë“œ ìœ ì§€)\n",
    "                    if loss.item() < best_val_loss - 1e-6:\n",
    "                        best_val_loss = loss.item()\n",
    "                        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ë° ì €ì¥\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    save_model_and_ct(best_model_state, ct, MODEL_DIR_I, run, label, input_dim=X_train.shape[1])\n",
    "\n",
    "                # ë‚´ë¶€ í‰ê°€\n",
    "                surv_train = predict_survival(model, X_train_tensor, time_points)\n",
    "                surv_val   = predict_survival(model, X_val_tensor,   time_points)\n",
    "\n",
    "                auc_train = calc_auc(surv_train, y_train.reset_index(drop=True), time_points)\n",
    "                auc_val   = calc_auc(surv_val,   y_val.reset_index(drop=True),   time_points)\n",
    "                auc_train_list.append(auc_train)\n",
    "                auc_val_list.append(auc_val)\n",
    "\n",
    "                risk_train = surv_train.T\n",
    "                risk_val   = surv_val.T\n",
    "                cidx_train = [safe_concordance_index(y_train['time'], risk_train[:, j], y_train['event']) for j in range(len(time_points))]\n",
    "                cidx_val   = [safe_concordance_index(y_val['time'],   risk_val[:,   j], y_val['event'])   for j in range(len(time_points))]\n",
    "                cidx_train_list.append(cidx_train)\n",
    "                cidx_val_list.append(cidx_val)\n",
    "\n",
    "                # RAW ì €ì¥ìš© ëˆ„ì \n",
    "                for j, t in enumerate(time_points):\n",
    "                    raw_rows_auc.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": t,\n",
    "                                         \"AUC (Train)\": auc_train[t], \"AUC (Val)\": auc_val[t], \"Scope\": \"Time-wise\"})\n",
    "                    raw_rows_cidx.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": t,\n",
    "                                          \"C-index (Train)\": cidx_train[j], \"C-index (Val)\": cidx_val[j], \"Scope\": \"Time-wise\"})\n",
    "                raw_rows_auc.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": \"Overall\",\n",
    "                                     \"AUC (Train)\": np.nanmean(list(auc_train.values())),\n",
    "                                     \"AUC (Val)\":   np.nanmean(list(auc_val.values())), \"Scope\": \"Overall\"})\n",
    "                raw_rows_cidx.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": \"Overall\",\n",
    "                                      \"C-index (Train)\": np.nanmean(cidx_train),\n",
    "                                      \"C-index (Val)\":   np.nanmean(cidx_val), \"Scope\": \"Overall\"})\n",
    "\n",
    "            # ìš”ì•½ í†µê³„\n",
    "            results_dict[label] = {\n",
    "                'mean_auc_train': {t: np.nanmean([r[t] for r in auc_train_list]) for t in time_points},\n",
    "                'mean_auc_val':   {t: np.nanmean([r[t] for r in auc_val_list])   for t in time_points},\n",
    "                'std_auc_train':  {t: np.nanstd([r[t] for r in auc_train_list])  for t in time_points},\n",
    "                'std_auc_val':    {t: np.nanstd([r[t] for r in auc_val_list])    for t in time_points},\n",
    "                'mean_cidx_train':{t: np.nanmean([r[j] for r in cidx_train_list]) for j, t in enumerate(time_points)},\n",
    "                'mean_cidx_val':  {t: np.nanmean([r[j] for r in cidx_val_list])   for j, t in enumerate(time_points)},\n",
    "                'std_cidx_train': {t: np.nanstd([r[j] for r in cidx_train_list])  for j, t in enumerate(time_points)},\n",
    "                'std_cidx_val':   {t: np.nanstd([r[j] for r in cidx_val_list])    for j, t in enumerate(time_points)}\n",
    "            }\n",
    "\n",
    "        # ê²°ê³¼ CSV ì €ì¥\n",
    "        raw_auc_path  = os.path.join(SAVE_ROOT, f\"raw_auc_per_time_run{ONLY_RUN_IDX:02d}.csv\")\n",
    "        raw_cidx_path = os.path.join(SAVE_ROOT, f\"raw_cindex_per_time_run{ONLY_RUN_IDX:02d}.csv\")\n",
    "        pd.DataFrame(raw_rows_auc).to_csv(raw_auc_path, index=False)\n",
    "        pd.DataFrame(raw_rows_cidx).to_csv(raw_cidx_path, index=False)\n",
    "        print(f\"âœ… ë‚´ë¶€ í‰ê°€ ì €ì¥ ì™„ë£Œ: run{ONLY_RUN_IDX:02d}\")\n",
    "\n",
    "        # ì‹œê°í™” ì €ì¥ (AUC)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for label in feature_sets:\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_auc_train'].values()),\n",
    "                         yerr=list(results_dict[label]['std_auc_train'].values()),\n",
    "                         fmt='--o', capsize=4, label=f\"{label} - AUC Train\")\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_auc_val'].values()),\n",
    "                         yerr=list(results_dict[label]['std_auc_val'].values()),\n",
    "                         fmt='-o', capsize=4, label=f\"{label} - AUC Val\")\n",
    "        plt.title(f\"AUC (Run {ONLY_RUN_IDX:02d})\")\n",
    "        plt.xlabel(\"Time (Months)\")\n",
    "        plt.ylabel(\"AUC\")\n",
    "        plt.ylim(0.1, 1.0)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_ROOT, f\"plot_auc_run{ONLY_RUN_IDX:02d}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # ì‹œê°í™” ì €ì¥ (C-index)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for label in feature_sets:\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_cidx_train'].values()),\n",
    "                         yerr=list(results_dict[label]['std_cidx_train'].values()),\n",
    "                         fmt='--s', capsize=4, label=f\"{label} - C-index Train\")\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_cidx_val'].values()),\n",
    "                         yerr=list(results_dict[label]['std_cidx_val'].values()),\n",
    "                         fmt='-s', capsize=4, label=f\"{label} - C-index Val\")\n",
    "        plt.title(f\"C-index (Run {ONLY_RUN_IDX:02d})\")\n",
    "        plt.xlabel(\"Time (Months)\")\n",
    "        plt.ylabel(\"C-index\")\n",
    "        plt.ylim(0.1, 1.0)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_ROOT, f\"plot_cindex_run{ONLY_RUN_IDX:02d}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # =========================\n",
    "        # ì™¸ë¶€ ë°ì´í„° í‰ê°€ (í•´ë‹¹ iì—ì„œ ì €ì¥ëœ ëª¨ë“  run Ã— Feature Set)\n",
    "        # =========================\n",
    "        ext_auc_rows, ext_cidx_rows = evaluate_external_for_all_models(\n",
    "            MODEL_DIR=MODEL_DIR_I,\n",
    "            EXTERNAL_CSV=EXTERNAL_CSV,\n",
    "            time_points=time_points,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if ext_auc_rows:\n",
    "            ext_auc_path  = os.path.join(SAVE_ROOT, f\"external_auc_ALL_runs_from_file{ONLY_RUN_IDX:02d}.csv\")\n",
    "            pd.DataFrame(ext_auc_rows).to_csv(ext_auc_path, index=False)\n",
    "        if ext_cidx_rows:\n",
    "            ext_cidx_path = os.path.join(SAVE_ROOT, f\"external_cindex_ALL_runs_from_file{ONLY_RUN_IDX:02d}.csv\")\n",
    "            pd.DataFrame(ext_cidx_rows).to_csv(ext_cidx_path, index=False)\n",
    "\n",
    "        if (ext_auc_rows and len(ext_auc_rows)) or (ext_cidx_rows and len(ext_cidx_rows)):\n",
    "            print(\"âœ… ì™¸ë¶€ í‰ê°€(ëª¨ë“  ì €ì¥ ëª¨ë¸) ì €ì¥ ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ê±°ë‚˜, ì™¸ë¶€ ë°ì´í„° ì»¬ëŸ¼ ëˆ„ë½ìœ¼ë¡œ í‰ê°€ê°€ ìŠ¤í‚µë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
