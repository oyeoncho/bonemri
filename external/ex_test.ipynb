{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture Stretched Exponential Survival with Debugging, NaN Handling, Model Saving, and External Evaluation\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lifelines.utils import concordance_index\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "# =========================\n",
    "# Config (상단 설정 일부만 수정)\n",
    "# =========================\n",
    "BASE_GROUPS = [\"beit0\"]\n",
    "GROUP = \"n7_30_30\"\n",
    "N_RUNS = 30\n",
    "time_points = [12, 24, 36, 48, 60, 72]\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# ✅ 전역 재현성 고정용 기본 시드\n",
    "BASE_SEED = 20250903\n",
    "\n",
    "# 저장 디렉토리\n",
    "os.makedirs(\"./survival_model/mixture_non_fix/models\", exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "\n",
    "def evaluate_external_for_all_models(MODEL_DIR, EXTERNAL_CSV, time_points, device=DEVICE):\n",
    "    if not os.path.exists(EXTERNAL_CSV):\n",
    "        print(f\"ℹ️ 외부 평가 스킵 (파일 없음): {EXTERNAL_CSV}\")\n",
    "        return None, None\n",
    "\n",
    "    df_ext = pd.read_csv(EXTERNAL_CSV)\n",
    "\n",
    "    # 외부 데이터 event/time 컬럼 보정\n",
    "    if 'event' not in df_ext.columns and 'survival' in df_ext.columns:\n",
    "        df_ext['event'] = df_ext['survival'].astype(int)\n",
    "    if 'time' not in df_ext.columns and 'fu_date' in df_ext.columns:\n",
    "        df_ext['time'] = df_ext['fu_date'].astype(np.float32)\n",
    "\n",
    "    # MODEL_DIR 내 모든 체크포인트 스캔 (예: best_model_run13_Image_only.pt)\n",
    "    ckpt_paths = glob.glob(os.path.join(MODEL_DIR, \"best_model_run*.pt\"))\n",
    "    if not ckpt_paths:\n",
    "        print(f\"⚠️ {MODEL_DIR} 에 저장된 모델이 없습니다.\")\n",
    "        return None, None\n",
    "\n",
    "    pattern = re.compile(r\"best_model_run(\\d+)_([^\\.]+)\\.pt\")\n",
    "    ext_rows_auc_all, ext_rows_cidx_all = [], []\n",
    "\n",
    "    for ckpt_path in sorted(ckpt_paths):\n",
    "        m = pattern.search(os.path.basename(ckpt_path))\n",
    "        if not m:\n",
    "            print(f\"⚠️ 스킵(파일명 파싱 실패): {ckpt_path}\")\n",
    "            continue\n",
    "        run_idx = int(m.group(1))\n",
    "        label_raw = m.group(2)                  # e.g., Image_only\n",
    "        label = label_raw.replace('_', ' ')     # \"Image only\"\n",
    "\n",
    "        # ColumnTransformer 경로\n",
    "        ct_path = os.path.join(MODEL_DIR, f\"ct_run{run_idx:02d}_{label_raw}.joblib\")\n",
    "        if not os.path.exists(ct_path):\n",
    "            print(f\"⚠️ 전처리기 누락 → 스킵: {ct_path}\")\n",
    "            continue\n",
    "\n",
    "        # 모델/CT 로드\n",
    "        try:\n",
    "            model, ct = load_model_and_ct(MODEL_DIR, run_idx, label, device=device, num_components=2)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️ 로드 실패 → 스킵: run{run_idx:02d}, {label}\")\n",
    "            continue\n",
    "\n",
    "        # ColumnTransformer가 기대하는 **원본 입력 컬럼** 모으기\n",
    "        required_columns = []\n",
    "        for name, trans, cols in ct.transformers_:\n",
    "            if cols is None or cols == []:\n",
    "                continue\n",
    "            if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "                required_columns.extend(list(cols))\n",
    "            else:\n",
    "                required_columns.append(cols)\n",
    "\n",
    "        missing = [c for c in required_columns if c not in df_ext.columns]\n",
    "        if missing:\n",
    "            print(f\"⚠️ 외부 데이터에 '{label}' 입력 컬럼 누락: {missing} → 스킵(run{run_idx:02d})\")\n",
    "            continue\n",
    "\n",
    "        # 변환 및 예측\n",
    "        X_ext = ct.transform(df_ext[required_columns])\n",
    "        X_ext = pd.DataFrame(X_ext).fillna(0).values.astype(np.float32)\n",
    "        X_ext_tensor = torch.tensor(X_ext, dtype=torch.float32).to(device)\n",
    "\n",
    "        surv_ext = predict_survival(model, X_ext_tensor, time_points)  # (T, N)\n",
    "        y_ext = df_ext[['time', 'event']].copy()\n",
    "\n",
    "        # AUC / C-index\n",
    "        auc_ext = calc_auc(surv_ext, y_ext.reset_index(drop=True), time_points)\n",
    "        risk_ext = surv_ext.T\n",
    "        cidx_ext = [safe_concordance_index(y_ext['time'], risk_ext[:, j], y_ext['event'])\n",
    "                    for j in range(len(time_points))]\n",
    "\n",
    "        # 누적 저장\n",
    "        for j, t in enumerate(time_points):\n",
    "            ext_rows_auc_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                     \"Time (Months)\": t, \"AUC (External)\": auc_ext[t]})\n",
    "            ext_rows_cidx_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                      \"Time (Months)\": t, \"C-index (External)\": cidx_ext[j]})\n",
    "        ext_rows_auc_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                 \"Time (Months)\": \"Overall\",\n",
    "                                 \"AUC (External)\": np.nanmean(list(auc_ext.values()))})\n",
    "        ext_rows_cidx_all.append({\"RunFile\": f\"run{run_idx:02d}\", \"Feature Set\": label,\n",
    "                                  \"Time (Months)\": \"Overall\",\n",
    "                                  \"C-index (External)\": np.nanmean(cidx_ext)})\n",
    "\n",
    "    return ext_rows_auc_all, ext_rows_cidx_all\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "class MixtureStretchedExponentialSurvival(nn.Module):\n",
    "    def __init__(self, input_dim, num_components=2):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU()\n",
    "        )\n",
    "        self.pi_layer = nn.Linear(64, num_components)\n",
    "        self.lam_layer = nn.Linear(64, num_components)\n",
    "        self.alpha_layer = nn.Linear(64, num_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        pi = F.softmax(self.pi_layer(h), dim=1)\n",
    "        lam = F.softplus(self.lam_layer(h)) + 1e-3\n",
    "        a = F.softplus(self.alpha_layer(h)) + 1e-3\n",
    "        return pi, lam, a\n",
    "\n",
    "def mixture_stretched_nll(t, e, pi, lam, a, eps=1e-8):\n",
    "    t = t.view(-1, 1)\n",
    "    t_a = torch.pow(t + eps, a)\n",
    "    S_k = torch.exp(-lam * t_a)\n",
    "    f_k = lam * a * torch.pow(t + eps, a - 1) * S_k\n",
    "    f = torch.sum(pi * f_k, dim=1) + eps\n",
    "    S = torch.sum(pi * S_k, dim=1) + eps\n",
    "    loglik = e * torch.log(f) + (1 - e) * torch.log(S)\n",
    "    return -loglik.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_survival(model, x, times):\n",
    "    model.eval()\n",
    "    pi, lam, a = model(x)\n",
    "    surv = []\n",
    "    for t in times:\n",
    "        t_tensor = torch.tensor([t], dtype=torch.float32, device=x.device)\n",
    "        t_a = torch.pow(t_tensor + 1e-8, a)\n",
    "        S_k = torch.exp(-lam * t_a)\n",
    "        S = torch.sum(pi * S_k, dim=1)\n",
    "        surv.append(S.cpu().numpy())\n",
    "    return np.vstack(surv)  # shape: (len(times), N)\n",
    "\n",
    "def calc_auc(surv_arr, y_df, times):\n",
    "    aucs = {}\n",
    "    for i, t in enumerate(times):\n",
    "        true = ((y_df[\"event\"] == 1) & (y_df[\"time\"] <= t)).astype(int)\n",
    "        pred = 1 - surv_arr[i, :]\n",
    "        try:\n",
    "            aucs[t] = roc_auc_score(true, pred)\n",
    "        except Exception:\n",
    "            aucs[t] = np.nan\n",
    "    return aucs\n",
    "\n",
    "def safe_concordance_index(times, risks, events):\n",
    "    times = np.asarray(times)\n",
    "    risks = np.asarray(risks)\n",
    "    events = np.asarray(events)\n",
    "    mask = ~(np.isnan(times) | np.isnan(risks) | np.isnan(events))\n",
    "    if np.sum(mask) < 2:\n",
    "        print(\"⚠️ Too few valid samples for C-index:\", np.sum(mask))\n",
    "        return np.nan\n",
    "    if np.std(risks[mask]) < 1e-6:\n",
    "        print(\"⚠️ Low risk variance, skipping C-index\")\n",
    "        return np.nan\n",
    "    return concordance_index(times[mask], risks[mask], events[mask])\n",
    "\n",
    "def save_model_and_ct(model_state, ct, save_dir, run_idx, label, input_dim):\n",
    "    tag = f\"run{run_idx:02d}_{label.replace(' ', '_')}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save({\n",
    "        \"state_dict\": model_state,\n",
    "        \"input_dim\": input_dim\n",
    "    }, os.path.join(save_dir, f\"best_model_{tag}.pt\"))\n",
    "    joblib.dump(ct, os.path.join(save_dir, f\"ct_{tag}.joblib\"))\n",
    "\n",
    "def load_model_and_ct(model_dir, run_idx, label, device=DEVICE, num_components=2):\n",
    "    tag = f\"run{run_idx:02d}_{label.replace(' ', '_')}\"\n",
    "    ckpt = torch.load(os.path.join(model_dir, f\"best_model_{tag}.pt\"), map_location=device)\n",
    "    input_dim = ckpt[\"input_dim\"]\n",
    "    model = MixtureStretchedExponentialSurvival(input_dim=input_dim, num_components=num_components).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    ct = joblib.load(os.path.join(model_dir, f\"ct_{tag}.joblib\"))\n",
    "    model.eval()\n",
    "    return model, ct\n",
    "\n",
    "def evaluate_on_dataframe(model, ct, df, feature_cols, time_points, device=DEVICE):\n",
    "    X_df = df[feature_cols].copy()\n",
    "    X = ct.transform(X_df)\n",
    "    X = pd.DataFrame(X).fillna(0).values.astype(np.float32)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    surv = predict_survival(model, X_tensor, time_points)  # (T, N)\n",
    "    return surv\n",
    "# =========================\n",
    "# Main (이 블록 전체 교체)\n",
    "# =========================\n",
    "device = DEVICE\n",
    "\n",
    "for base_group in BASE_GROUPS:\n",
    "    print(f\"\\n\\n============================\")\n",
    "    print(f\"📁 BEiT 그룹 실행 중: {base_group}\")\n",
    "    print(f\"============================\")\n",
    "\n",
    "    SAVE_ROOT_BASE = f\"./survival_model/mixture_non_fix/tune/{base_group}/results/generalization/test0/dl0/{GROUP}\"\n",
    "    MODEL_DIR_ROOT = f\"./survival_model/mixture_non_fix/models/{base_group}/{GROUP}\"\n",
    "    os.makedirs(SAVE_ROOT_BASE, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR_ROOT, exist_ok=True)\n",
    "\n",
    "    # 공통 컬럼 설정\n",
    "    img_cols = [\"feat_436\", \"feat_519\"]\n",
    "    cont_cols = ['Age', 'stage0']\n",
    "    cat_cols  = ['pathology']\n",
    "\n",
    "    feature_sets = {\n",
    "        'Image only': (img_cols, []),\n",
    "        'Clinical only': ([], cont_cols + cat_cols),\n",
    "        'Image + Clinical': (img_cols, cont_cols + cat_cols)\n",
    "    }\n",
    "\n",
    "    # ✅ ONLY_RUN_IDX = 1..30 & EXTERNAL_CSV = ./external/external{idx}.csv 로 반복\n",
    "    for i in range(1, 31):\n",
    "        ONLY_RUN_IDX = i\n",
    "        EXTERNAL_CSV = f\"./external/external{ONLY_RUN_IDX}.csv\"\n",
    "\n",
    "        fname = f\"dh11_run{ONLY_RUN_IDX:02d}.csv\"\n",
    "        csv_path = f\"./deephit/{base_group}/test/dl0/{GROUP}/{fname}\"\n",
    "        print(f\"\\n🚀 실행 중: {base_group} - {fname} | EXTERNAL: external{ONLY_RUN_IDX}.csv\")\n",
    "\n",
    "        # ✅ i별 폴더\n",
    "        MODEL_DIR_I = os.path.join(MODEL_DIR_ROOT, f\"file{ONLY_RUN_IDX:02d}\")\n",
    "        SAVE_ROOT   = os.path.join(SAVE_ROOT_BASE, f\"file{ONLY_RUN_IDX:02d}\")\n",
    "        os.makedirs(MODEL_DIR_I, exist_ok=True)\n",
    "        os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"⚠️ 내부 데이터 없음 → 스킵: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        df_all = pd.read_csv(csv_path)\n",
    "        # event/time 열 매핑\n",
    "        if 'event' not in df_all.columns and 'survival' in df_all.columns:\n",
    "            df_all['event'] = df_all['survival'].astype(int)\n",
    "        if 'time' not in df_all.columns and 'fu_date' in df_all.columns:\n",
    "            df_all['time']  = df_all['fu_date'].astype(np.float32)\n",
    "\n",
    "        results_dict = {}\n",
    "        raw_rows_auc, raw_rows_cidx = [], []\n",
    "\n",
    "        for label, (img_part, clinical_part) in feature_sets.items():\n",
    "            print(f\"\\n📌 Feature Set: {label}\")\n",
    "            auc_train_list, auc_val_list = [], []\n",
    "            cidx_train_list, cidx_val_list = [], []\n",
    "\n",
    "            # ----- Monte Carlo N_RUNS -----\n",
    "            for run in range(N_RUNS):\n",
    "                # ✅ 시드 고정: 전역 기본 시드 + run (항상 동일 재현)\n",
    "                set_seed(BASE_SEED + run)\n",
    "\n",
    "                # Design X / y\n",
    "                used_cols = img_part + clinical_part\n",
    "                X_df = df_all[used_cols].copy()\n",
    "                y_df = df_all[['time', 'event']].copy()\n",
    "\n",
    "                X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "                    X_df, y_df, test_size=0.3, random_state=BASE_SEED + run\n",
    "                )\n",
    "\n",
    "                transformers = []\n",
    "                if img_part:\n",
    "                    transformers.append(('img', StandardScaler(), img_part))\n",
    "                cont = [c for c in clinical_part if c in cont_cols]\n",
    "                cat  = [c for c in clinical_part if c in cat_cols]\n",
    "                if cont:\n",
    "                    transformers.append(('cont', StandardScaler(), cont))\n",
    "                if cat:\n",
    "                    transformers.append(('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat))\n",
    "\n",
    "                ct = ColumnTransformer(transformers)\n",
    "                X_train = ct.fit_transform(X_train_df)\n",
    "                X_val   = ct.transform(X_val_df)\n",
    "\n",
    "                X_train = pd.DataFrame(X_train).fillna(0).values.astype(np.float32)\n",
    "                X_val   = pd.DataFrame(X_val).fillna(0).values.astype(np.float32)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "                X_val_tensor   = torch.tensor(X_val,   dtype=torch.float32).to(device)\n",
    "                t_train = torch.tensor(y_train['time'].values,  dtype=torch.float32).to(device)\n",
    "                e_train = torch.tensor(y_train['event'].values, dtype=torch.float32).to(device)\n",
    "\n",
    "                model = MixtureStretchedExponentialSurvival(\n",
    "                    input_dim=X_train.shape[1], num_components=2\n",
    "                ).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "                best_val_loss = float('inf')\n",
    "                patience, patience_counter = 10, 0\n",
    "                best_model_state = None\n",
    "\n",
    "                for epoch in range(1000):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    pi, lam, a = model(X_train_tensor)\n",
    "                    loss = mixture_stretched_nll(t_train, e_train, pi, lam, a)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # 간단 Early Stopping (train loss 기준, 원 코드 유지)\n",
    "                    if loss.item() < best_val_loss - 1e-6:\n",
    "                        best_val_loss = loss.item()\n",
    "                        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                # 베스트 모델 로드 및 저장\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    save_model_and_ct(best_model_state, ct, MODEL_DIR_I, run, label, input_dim=X_train.shape[1])\n",
    "\n",
    "                # 내부 평가\n",
    "                surv_train = predict_survival(model, X_train_tensor, time_points)\n",
    "                surv_val   = predict_survival(model, X_val_tensor,   time_points)\n",
    "\n",
    "                auc_train = calc_auc(surv_train, y_train.reset_index(drop=True), time_points)\n",
    "                auc_val   = calc_auc(surv_val,   y_val.reset_index(drop=True),   time_points)\n",
    "                auc_train_list.append(auc_train)\n",
    "                auc_val_list.append(auc_val)\n",
    "\n",
    "                risk_train = surv_train.T\n",
    "                risk_val   = surv_val.T\n",
    "                cidx_train = [safe_concordance_index(y_train['time'], risk_train[:, j], y_train['event']) for j in range(len(time_points))]\n",
    "                cidx_val   = [safe_concordance_index(y_val['time'],   risk_val[:,   j], y_val['event'])   for j in range(len(time_points))]\n",
    "                cidx_train_list.append(cidx_train)\n",
    "                cidx_val_list.append(cidx_val)\n",
    "\n",
    "                # RAW 저장용 누적\n",
    "                for j, t in enumerate(time_points):\n",
    "                    raw_rows_auc.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": t,\n",
    "                                         \"AUC (Train)\": auc_train[t], \"AUC (Val)\": auc_val[t], \"Scope\": \"Time-wise\"})\n",
    "                    raw_rows_cidx.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": t,\n",
    "                                          \"C-index (Train)\": cidx_train[j], \"C-index (Val)\": cidx_val[j], \"Scope\": \"Time-wise\"})\n",
    "                raw_rows_auc.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": \"Overall\",\n",
    "                                     \"AUC (Train)\": np.nanmean(list(auc_train.values())),\n",
    "                                     \"AUC (Val)\":   np.nanmean(list(auc_val.values())), \"Scope\": \"Overall\"})\n",
    "                raw_rows_cidx.append({\"Feature Set\": label, \"Run\": run, \"Time (Months)\": \"Overall\",\n",
    "                                      \"C-index (Train)\": np.nanmean(cidx_train),\n",
    "                                      \"C-index (Val)\":   np.nanmean(cidx_val), \"Scope\": \"Overall\"})\n",
    "\n",
    "            # 요약 통계\n",
    "            results_dict[label] = {\n",
    "                'mean_auc_train': {t: np.nanmean([r[t] for r in auc_train_list]) for t in time_points},\n",
    "                'mean_auc_val':   {t: np.nanmean([r[t] for r in auc_val_list])   for t in time_points},\n",
    "                'std_auc_train':  {t: np.nanstd([r[t] for r in auc_train_list])  for t in time_points},\n",
    "                'std_auc_val':    {t: np.nanstd([r[t] for r in auc_val_list])    for t in time_points},\n",
    "                'mean_cidx_train':{t: np.nanmean([r[j] for r in cidx_train_list]) for j, t in enumerate(time_points)},\n",
    "                'mean_cidx_val':  {t: np.nanmean([r[j] for r in cidx_val_list])   for j, t in enumerate(time_points)},\n",
    "                'std_cidx_train': {t: np.nanstd([r[j] for r in cidx_train_list])  for j, t in enumerate(time_points)},\n",
    "                'std_cidx_val':   {t: np.nanstd([r[j] for r in cidx_val_list])    for j, t in enumerate(time_points)}\n",
    "            }\n",
    "\n",
    "        # 결과 CSV 저장\n",
    "        raw_auc_path  = os.path.join(SAVE_ROOT, f\"raw_auc_per_time_run{ONLY_RUN_IDX:02d}.csv\")\n",
    "        raw_cidx_path = os.path.join(SAVE_ROOT, f\"raw_cindex_per_time_run{ONLY_RUN_IDX:02d}.csv\")\n",
    "        pd.DataFrame(raw_rows_auc).to_csv(raw_auc_path, index=False)\n",
    "        pd.DataFrame(raw_rows_cidx).to_csv(raw_cidx_path, index=False)\n",
    "        print(f\"✅ 내부 평가 저장 완료: run{ONLY_RUN_IDX:02d}\")\n",
    "\n",
    "        # 시각화 저장 (AUC)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for label in feature_sets:\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_auc_train'].values()),\n",
    "                         yerr=list(results_dict[label]['std_auc_train'].values()),\n",
    "                         fmt='--o', capsize=4, label=f\"{label} - AUC Train\")\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_auc_val'].values()),\n",
    "                         yerr=list(results_dict[label]['std_auc_val'].values()),\n",
    "                         fmt='-o', capsize=4, label=f\"{label} - AUC Val\")\n",
    "        plt.title(f\"AUC (Run {ONLY_RUN_IDX:02d})\")\n",
    "        plt.xlabel(\"Time (Months)\")\n",
    "        plt.ylabel(\"AUC\")\n",
    "        plt.ylim(0.1, 1.0)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_ROOT, f\"plot_auc_run{ONLY_RUN_IDX:02d}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # 시각화 저장 (C-index)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for label in feature_sets:\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_cidx_train'].values()),\n",
    "                         yerr=list(results_dict[label]['std_cidx_train'].values()),\n",
    "                         fmt='--s', capsize=4, label=f\"{label} - C-index Train\")\n",
    "            plt.errorbar(time_points, list(results_dict[label]['mean_cidx_val'].values()),\n",
    "                         yerr=list(results_dict[label]['std_cidx_val'].values()),\n",
    "                         fmt='-s', capsize=4, label=f\"{label} - C-index Val\")\n",
    "        plt.title(f\"C-index (Run {ONLY_RUN_IDX:02d})\")\n",
    "        plt.xlabel(\"Time (Months)\")\n",
    "        plt.ylabel(\"C-index\")\n",
    "        plt.ylim(0.1, 1.0)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_ROOT, f\"plot_cindex_run{ONLY_RUN_IDX:02d}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # =========================\n",
    "        # 외부 데이터 평가 (해당 i에서 저장된 모든 run × Feature Set)\n",
    "        # =========================\n",
    "        ext_auc_rows, ext_cidx_rows = evaluate_external_for_all_models(\n",
    "            MODEL_DIR=MODEL_DIR_I,\n",
    "            EXTERNAL_CSV=EXTERNAL_CSV,\n",
    "            time_points=time_points,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if ext_auc_rows:\n",
    "            ext_auc_path  = os.path.join(SAVE_ROOT, f\"external_auc_ALL_runs_from_file{ONLY_RUN_IDX:02d}.csv\")\n",
    "            pd.DataFrame(ext_auc_rows).to_csv(ext_auc_path, index=False)\n",
    "        if ext_cidx_rows:\n",
    "            ext_cidx_path = os.path.join(SAVE_ROOT, f\"external_cindex_ALL_runs_from_file{ONLY_RUN_IDX:02d}.csv\")\n",
    "            pd.DataFrame(ext_cidx_rows).to_csv(ext_cidx_path, index=False)\n",
    "\n",
    "        if (ext_auc_rows and len(ext_auc_rows)) or (ext_cidx_rows and len(ext_cidx_rows)):\n",
    "            print(\"✅ 외부 평가(모든 저장 모델) 저장 완료\")\n",
    "        else:\n",
    "            print(\"ℹ️ 저장된 모델이 없거나, 외부 데이터 컬럼 누락으로 평가가 스킵되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf34a0",
   "metadata": {},
   "source": [
    "[\"feat_213\", \"feat_194\", \"feat_163\", \"feat_266\", \"feat_407\", \"feat_468\", \"feat_499\", \"feat_169\", \"feat_436\", \"feat_560\", \n",
    "                    \"feat_2\", \"feat_327\", \"feat_391\", \"feat_519\", \"feat_173\", \"feat_181\", \"feat_389\", \"feat_715\", \"feat_107\", \"feat_203\", \n",
    "                    \"feat_361\", \"feat_439\", \"feat_451\", \"feat_565\", \"feat_747\", \"feat_80\", \"feat_10\", \"feat_123\", \"feat_137\", \"feat_15\", \n",
    "                    \"feat_209\", \"feat_215\", \"feat_289\", \"feat_368\", \"feat_374\", \"feat_55\", \"feat_576\", \"feat_578\", \"feat_121\", \"feat_125\", \n",
    "                    \"feat_143\", \"feat_223\", \"feat_240\", \"feat_25\", \"feat_309\", \"feat_498\", \"feat_514\", \"feat_554\", \"feat_577\", \"feat_617\", \n",
    "                    \"feat_653\", \"feat_710\", \"feat_109\", \"feat_210\", \"feat_220\", \"feat_352\", \"feat_420\", \"feat_507\", \"feat_583\", \"feat_605\", \n",
    "                    \"feat_657\", \"feat_666\", \"feat_152\", \"feat_167\", \"feat_255\", \"feat_328\", \"feat_378\", \"feat_402\", \"feat_633\", \"feat_656\"]\n",
    "\n",
    "[\"feat_213\", \"feat_266\", \"feat_499\", \"feat_436\", \"feat_2\", \"feat_327\", \"feat_391\", \"feat_519\", \"feat_173\", \"feat_715\",\n",
    " \"feat_107\", \"feat_80\", \"feat_137\", \"feat_209\", \"feat_215\", \"feat_374\", \"feat_55\", \"feat_223\", \"feat_554\", \"feat_577\", \n",
    " \"feat_109\", \"feat_583\", \"feat_657\"]\n",
    "\n",
    "\n",
    "[\"feat_213\", \"feat_266\", \"feat_436\", \"feat_519\", \"feat_215\"]\n",
    "\n",
    "[\"feat_436\", \"feat_519\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
